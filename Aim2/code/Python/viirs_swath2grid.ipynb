{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e9667-ea08-4b6a-97d0-dec73a5242b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os, shutil, time, glob, warnings\n",
    "import earthaccess\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import h5py\n",
    "import pyproj\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from affine import Affine\n",
    "from pyresample import geometry as geom\n",
    "from pyresample import kd_tree as kdt\n",
    "from os.path import join\n",
    "from osgeo import gdal, gdal_array, gdalconst, osr\n",
    "from scipy.interpolate import RegularGridInterpolator as RGI\n",
    "\n",
    "# Explicitly use GDAL exceptions\n",
    "gdal.UseExceptions()\n",
    "\n",
    "# Projection information\n",
    "geog_crs = 'EPSG:4326'  # Geographic projection\n",
    "prj_crs = 'EPSG:5070'  # Projected coordinate system- WGS 84 NAD83 UTM Zone 13N\n",
    "\n",
    "# File path information\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/aspen-fire'\n",
    "datadir = os.path.join(maindir,'Aim2/data/spatial/raw/VIIRS/')\n",
    "\n",
    "# File path information\n",
    "print(\"Success !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a3566-b1cd-4ca3-8c29-4c157fbcd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert swath to grid\n",
    "\n",
    "def list_files(path, ext, recursive):\n",
    "    \"\"\"\n",
    "    List files of a specific type in a directory or subdirectories\n",
    "    \"\"\"\n",
    "    if recursive is True:\n",
    "        return glob.glob(os.path.join(path, '**', '*{}'.format(ext)), recursive=True)\n",
    "    else:\n",
    "        return glob.glob(os.path.join(path, '*{}'.format(ext)), recursive=False)\n",
    "\n",
    "\n",
    "def interpolate_geolocation(lat, lon, target_shape):\n",
    "    \"\"\" Interpolate the geolocation data to the target shape \"\"\"\n",
    "    lat_int = RGI((np.arange(lat.shape[0]), np.arange(lat.shape[1])), lat)\n",
    "    lon_int = RGI((np.arange(lon.shape[0]), np.arange(lon.shape[1])), lon)\n",
    "    target_coords = np.meshgrid(\n",
    "        np.linspace(0, lat.shape[0] - 1, target_shape[0]), \n",
    "        np.linspace(0, lon.shape[1] - 1, target_shape[1]), \n",
    "        indexing='ij')\n",
    "    target_coords = np.stack(target_coords, axis=-1)\n",
    "    lat_res = lat_int(target_coords)\n",
    "    lon_res = lon_int(target_coords)\n",
    "    return lat_res, lon_res\n",
    "\n",
    "\n",
    "def viirs_swath2grid(fireDA, geoDA, shortName, sdsName, ecoSDS, geomCoords, out_dir):\n",
    "    \"\"\" Converts VIIRS AFD NetCDF SDS to grid and exports as GeoTIFF \"\"\"\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - fireDA: The NetCDF file containing the fire information (*.nc)\n",
    "        - geoDA: The corresponding geolocation file (*.h5)\n",
    "        - shortName: The short name for the data product (e.g., VNP14IMG, VJ114IMG)\n",
    "        - sdsName: The name for the Science Dataset (SDS) (e.g., FP_power)\n",
    "        - geomBounds: the bounding geometry to create the output spatial array\n",
    "        - geomCoords: list of coordinate pairs, used to filter the data array\n",
    "    Returns:\n",
    "        - Spatial (projected) array for the given SDS and bounding geometry\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################################\n",
    "    # Open the geolocation file (.h5) and read contents (lat/lon SDS)\n",
    "    geo = h5py.File(geoDA)\n",
    "    geo_objs = []\n",
    "    geo.visit(geo_objs.append) # stores the SDS objects\n",
    "\n",
    "    # Get the file name \n",
    "    geoName = os.path.basename(geoDA).split('.h5')[0]\n",
    "\n",
    "    # Retrieve the coordinate SDS\n",
    "    latSD = [str(obj) for obj in geo_objs if isinstance(geo[obj], h5py.Dataset) and '/Latitude' in obj]\n",
    "    lonSD = [str(obj) for obj in geo_objs if isinstance(geo[obj], h5py.Dataset) and '/Longitude' in obj]\n",
    "    # Open coordinates as arrays\n",
    "    lat = geo[latSD[0]][()].astype(np.float32)\n",
    "    lon = geo[lonSD[0]][()].astype(np.float32)\n",
    "    print(f\"latGEO shape: {lat.shape}\\nlonGEO shape: {lon.shape}\\nData Type: {type(lat)}\")\n",
    "\n",
    "    dims = lat.shape # shape of the swath coordinate array\n",
    "\n",
    "    lat[lat == geo[latSD[0]].attrs['_FillValue']] = np.nan\n",
    "    lon[lon == geo[lonSD[0]].attrs['_FillValue']] = np.nan\n",
    "    \n",
    "    ############################\n",
    "    # Load data from NetCDF file\n",
    "    ds = Dataset(fireDA, 'r')\n",
    "\n",
    "    \n",
    "    # Grab the Fire Pixel information (sparse arrays representing only pixel locations of active fire detections)\n",
    "    FP_power = ds.variables['FP_power'][:]\n",
    "    FP_latitude = ds.variables['FP_latitude'][:]\n",
    "    FP_longitude = ds.variables['FP_longitude'][:]\n",
    "\n",
    "    # Grab the fire mask (full array)\n",
    "    fire_mask = ds.variables['fire mask'][:]\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"FP_power shape: {FP_power.shape}\") # see the sparse array\n",
    "    print(f\"FP_latitude shape: {FP_latitude.shape}\")\n",
    "    print(f\"FP_longitude shape: {FP_longitude.shape}\")\n",
    "    print(f\"Fire Mask shape: {fire_mask.shape}\") # see the full array\n",
    "\n",
    "    # Resample the latlon SDS shape to match the fire mask (750m geolocation to 375m)\n",
    "    lat_res, lon_res = interpolate_geolocation(lat, lon, fire_mask.shape)\n",
    "    print(f\"Resampled lat shape: {lat_res.shape}, Resampled lon shape: {lon_res.shape}\")\n",
    "    \n",
    "    # Create swath and area definition using coordinate arrays and projection information\n",
    "    swathDef = geom.SwathDefinition(lons=lon_res, lats=lat_res) # from 'pyresample' geom\n",
    "    epsg, proj, pName = '4326', 'latlong', 'Geographic'  # Set output projection to Geographic CRS\n",
    "    llLon, llLat, urLon, urLat = np.nanmin(lon_res), np.nanmin(lat_res), np.nanmax(lon_res), np.nanmax(lat_res)\n",
    "    areaExtent = (llLon, llLat, urLon, urLat)\n",
    "    projDict = {'proj': proj, 'datum': 'WGS84'}\n",
    "\n",
    "    # Calculate the pixel dimensions, cols, and rows\n",
    "    ps = np.min([abs(areaExtent[2] - areaExtent[0]) / fire_mask.shape[1],\n",
    "                 abs(areaExtent[3] - areaExtent[1]) / fire_mask.shape[0]]) \n",
    "    # ps = 0.00333663072035137202  # Hard-coded estimate of pixel size in degrees\n",
    "    cols = int(round((areaExtent[2] - areaExtent[0]) / ps))  # Calculate the output cols\n",
    "    rows = int(round((areaExtent[3] - areaExtent[1]) / ps))  # Calculate the output rows\n",
    "\n",
    "    print(f\"Pixel Dims: {ps};\\nNumber of columns: {cols};\\nNumber of rows: {rows}\")\n",
    "\n",
    "    # Define output geometry and set up resampling\n",
    "    areaDef = geom.AreaDefinition(epsg, pName, epsg, projDict, cols, rows, areaExtent) \n",
    "    index, outdex, indexArr, distArr = kdt.get_neighbour_info(swathDef, areaDef, 3750, neighbours=1)\n",
    "\n",
    "    print(f'Area Definition Shape: {areaDef.shape}')\n",
    "\n",
    "    # Perform kdtree resampling (swath 2 grid conversion) --- for the fire mask\n",
    "    fv = -9999\n",
    "    sdGEO = kdt.get_sample_from_neighbour_info('nn', areaDef.shape, fire_mask, index, outdex, indexArr, fill_value=fv)\n",
    "    \n",
    "    # Gather the geotransform definition\n",
    "    gt = [areaDef.area_extent[0], ps, 0, areaDef.area_extent[3], 0, -ps]\n",
    "\n",
    "    # Set up the GeoTIFF export\n",
    "    outDir = os.path.join(out_dir, f'georeferenced/{shortName}')\n",
    "    # Check the directory exists, make it if not\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "\n",
    "    # Set up output name\n",
    "    identifier_ = identifier.replace(\".\", \"_\")\n",
    "    outName = os.path.join(outDir, sdsName + '_' + identifier_ + '.tif')\n",
    "    print(\"output file:\\n{}\\n\".format(outName))\n",
    "    \n",
    "    # Get driver, specify dimensions, define and set output geotransform\n",
    "    height, width = sdGEO.shape  # Define geotiff dimensions\n",
    "    driv = gdal.GetDriverByName('GTiff')\n",
    "    dataType = gdal_array.NumericTypeCodeToGDALTypeCode(sdGEO.dtype)\n",
    "    d = driv.Create(outName, width, height, 1, dataType)\n",
    "    d.SetGeoTransform(gt)\n",
    "\n",
    "    # Create and set output projection, write output array data\n",
    "    # Define target SRS\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(int(epsg))\n",
    "    d.SetProjection(srs.ExportToWkt())\n",
    "    band = d.GetRasterBand(1)\n",
    "    band.WriteArray(sdGEO)\n",
    "\n",
    "    # Define fill value if it exists, if not, set to mask fill value\n",
    "    if fv is not None and fv != 'NaN':\n",
    "        band.SetNoDataValue(fv)\n",
    "    else:\n",
    "        try:\n",
    "            band.SetNoDataValue(sdGEO.fill_value)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    band.FlushCache()\n",
    "    d, band = None, None\n",
    "    \n",
    "\n",
    "def get_coords(geom, buffer):\n",
    "    \"\"\" Returns the bounding box coordinates for a given geometry(ies) and buffer \"\"\"\n",
    "    _geom = geom.copy()\n",
    "    _geom['geometry'] = _geom.geometry.buffer(buffer)\n",
    "    bounds = _geom.to_crs(geog_crs).unary_union.envelope # make sure it is in geographic coordinates\n",
    "    coords = list(bounds.exterior.coords)\n",
    "\n",
    "    del _geom, bounds\n",
    "    return coords\n",
    "    \n",
    "\n",
    "print(\"Function to process VIIRS NetCDF files is ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bce8bb-50a9-4d94-929a-a32dbd80fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for one fire\n",
    "testDir = os.path.join(datadir,'FIRED_3518')\n",
    "    \n",
    "# Get a list of geo files\n",
    "geo_files = list_files(testDir,\"*.h5\",recursive=True)\n",
    "print(geo_files[0])\n",
    "\n",
    "# Get list of fire data files\n",
    "vnp_files = list_files(testDir,\"VNP*.nc\",recursive=True)\n",
    "vj1_files = list_files(testDir,\"VJ1*.nc\",recursive=True)\n",
    "print(vnp_files[0])\n",
    "print(vj1_files[0])\n",
    "\n",
    "# Create a dictionary to store the file paths\n",
    "datadict = {\n",
    "    'VNP14IMG': vnp_files,\n",
    "    'VJ114IMG': vj1_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8b8e6-6cdf-486e-ae63-6cdb78b2b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fire data and create a dictionary with bounding coordinates\n",
    "fires_path = os.path.join(maindir,'Aim2/data/spatial/mod/FIRED/fired_events_west_aspen.gpkg')\n",
    "fires = gpd.read_file(fires_path)\n",
    "print(fires.columns)\n",
    "print(len(fires))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e769d-bd0a-4fdf-b209-256add7d6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store fire bounding coordinates\n",
    "coords_dict = {}\n",
    "buffer = 375 \n",
    "\n",
    "for index, row in fires.iterrows():\n",
    "    fire_id = row['fired_id']\n",
    "    perim = fires.loc[fires['fired_id'] == fire_id]\n",
    "    coords = get_coords(perim, buffer)\n",
    "    coords_dict[fire_id] = coords\n",
    "\n",
    "# Print the dictionary to verify\n",
    "first = next(iter(coords_dict.items()))\n",
    "print(f\"FIRED_ID: {first[0]}, \\nBounding Coordinates: \\n{first[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db720c6c-ed34-4b2e-8076-e3914d3eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "dat = 'fire mask' # the SDS we are extracting ...\n",
    "\n",
    "fired_id = '3518'\n",
    "coords_ = coords_dict[fired_id]\n",
    "\n",
    "out_dir = testDir\n",
    "\n",
    "for short_name, fpaths in datadict.items():\n",
    "    print(f\"Processing NetCDF files for {short_name}\")\n",
    "    # Retrieve the geolocations files corresponding to the short name\n",
    "    sh_code = short_name[:3] # the platform code (e.g., 'VNP')\n",
    "    _geo_files = [gf for gf in geo_files if sh_code in os.path.basename(gf)]\n",
    "    print(f\"There are {len(_geo_files)} associated geolocation files ...\")\n",
    "    for fp in fpaths:\n",
    "        identifier = os.path.basename(fp)[:-3]\n",
    "        print(identifier)\n",
    "\n",
    "        # Open the NetCDF file\n",
    "        ds = Dataset(fp, 'r', format='NETCDF4')  # Read in VIIRS AFD file\n",
    "\n",
    "        # Create a list of all SDS inside of the .nc file\n",
    "        ecoSDS = list(ds.variables.keys())\n",
    "\n",
    "        del ds # clean up !\n",
    "\n",
    "        # Find the matching ECO1BGEO file from the file list\n",
    "        parts = identifier.split('.')\n",
    "        if short_name == 'VNP14IMG':\n",
    "            date_time_part = '.'.join(parts[1:4])  # Extract date-time parts for the VNP Version 002\n",
    "        else:\n",
    "            date_time_part = '.'.join(parts[1:3])  \n",
    "        geo_identifier = sh_code + '03MODLL' + '.' + date_time_part\n",
    "        geo = [geo_link for geo_link in _geo_files if geo_identifier in os.path.basename(geo_link)]        \n",
    "        print(geo)\n",
    "\n",
    "        ###################################\n",
    "        # Now apply our processing function\n",
    "        viirs_swath2grid(\n",
    "            fireDA=fp, \n",
    "            geoDA=geo[0], \n",
    "            shortName=short_name, \n",
    "            sdsName=dat, \n",
    "            ecoSDS=ecoSDS, \n",
    "            geomCoords=coords_, \n",
    "            out_dir=out_dir\n",
    "        )\n",
    "        \n",
    "        print('Time to complete granule:', time.time() - t0)\n",
    "        print(\"\\n\")\n",
    "        print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffee50-31a9-4e7f-b0be-80b51c2d6e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c207a-70a6-423c-b516-e9efd687251e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24233a9a-2faf-4ce8-88cc-007bea1e7051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create FRP grid from daily granules\n",
    "def create_frp_grid(datadict, geo_files, coords_dict, out_dir):\n",
    "    \"\"\"\n",
    "    Create FRP grid from daily granules and eventually generate a maximum FRP grid for fire events.\n",
    "    \"\"\"\n",
    "    for fired_id, coords_ in coords_dict.items():\n",
    "        print(f\"Processing fire event {fired_id}\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Initialize an empty array to store the maximum FRP values\n",
    "        max_frp = None\n",
    "\n",
    "        for short_name, fpaths in datadict.items():\n",
    "            print(f\"Processing NetCDF files for {short_name}\")\n",
    "            sh_code = short_name[:3]  # the platform code (e.g., 'VNP')\n",
    "            _geo_files = [gf for gf in geo_files if sh_code in os.path.basename(gf)]\n",
    "            print(f\"There are {len(_geo_files)} associated geolocation files ...\")\n",
    "\n",
    "            for fp in fpaths:\n",
    "                identifier = os.path.basename(fp)[:-3]\n",
    "                print(identifier)\n",
    "\n",
    "                # Open the NetCDF file\n",
    "                ds = Dataset(fp, 'r', format='NETCDF4')  # Read in VIIRS AFD file\n",
    "\n",
    "                # Extract the FP_power SDS\n",
    "                FP_power = ds.variables['FP_power'][:]\n",
    "                fire_mask = ds.variables['fire mask'][:]\n",
    "\n",
    "                # Find the matching GEO file\n",
    "                parts = identifier.split('.')\n",
    "                if short_name == 'VNP14IMG':\n",
    "                    date_time_part = '.'.join(parts[1:4])  # Extract date-time parts for the VNP Version 002\n",
    "                else:\n",
    "                    date_time_part = '.'.join(parts[1:3])  \n",
    "                geo_identifier = sh_code + '03MODLL' + '.' + date_time_part\n",
    "                geo = [geo_link for geo_link in _geo_files if geo_identifier in os.path.basename(geo_link)]        \n",
    "                print(geo)\n",
    "\n",
    "                # Convert swath to grid\n",
    "                frp_grid = viirs_swath2grid(fp, geo[0], short_name, 'FP_power', ecoSDS=None, geomCoords=coords_, out_dir=out_dir)\n",
    "                \n",
    "                # Update the maximum FRP grid\n",
    "                if max_frp is None:\n",
    "                    max_frp = frp_grid\n",
    "                else:\n",
    "                    max_frp = np.maximum(max_frp, frp_grid)\n",
    "\n",
    "                print('Time to complete granule:', time.time() - t0)\n",
    "                print(\"\\n\")\n",
    "                print(\"---------------------------------------------\")\n",
    "\n",
    "        # Save the maximum FRP grid as a GeoTIFF\n",
    "        outName = os.path.join(out_dir, f'max_FRP_{fired_id}.tif')\n",
    "        save_geotiff(max_frp, outName, geo[0])\n",
    "\n",
    "        print(f\"Completed processing for fire event {fired_id} in {time.time() - t0} seconds\")\n",
    "\n",
    "# Function to save the FRP grid as a GeoTIFF\n",
    "def save_geotiff(array, out_name, geo_file):\n",
    "    \"\"\"\n",
    "    Save the FRP grid as a GeoTIFF file.\n",
    "    \"\"\"\n",
    "    geo = h5py.File(geo_file)\n",
    "    lat = geo['/Latitude'][()].astype(np.float32)\n",
    "    lon = geo['/Longitude'][()].astype(np.float32)\n",
    "    \n",
    "    ps = np.min([np.abs(lon[1] - lon[0]), np.abs(lat[1] - lat[0])])\n",
    "    gt = [np.min(lon), ps, 0, np.max(lat), 0, -ps]\n",
    "\n",
    "    # Get driver, specify dimensions, define and set output geotransform\n",
    "    height, width = array.shape  # Define geotiff dimensions\n",
    "    driv = gdal.GetDriverByName('GTiff')\n",
    "    dataType = gdal_array.NumericTypeCodeToGDALTypeCode(array.dtype)\n",
    "    d = driv.Create(out_name, width, height, 1, dataType)\n",
    "    d.SetGeoTransform(gt)\n",
    "\n",
    "    # Create and set output projection, write output array data\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)\n",
    "    d.SetProjection(srs.ExportToWkt())\n",
    "    band = d.GetRasterBand(1)\n",
    "    band.WriteArray(array)\n",
    "\n",
    "    band.FlushCache()\n",
    "    d, band = None, None\n",
    "    print(f\"Saved GeoTIFF: {out_name}\")\n",
    "\n",
    "# Test the function\n",
    "create_frp_grid(datadict, geo_files, coords_dict, testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b334ad-13be-4e2c-9618-3a28388ecb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
