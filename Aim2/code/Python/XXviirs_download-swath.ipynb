{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec03ae-2946-4baf-a5f7-f722a615f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Downloading VIIRS Active Fire Detections (AFD) with 'earthaccess' python API\n",
    "\n",
    "For a given geometry (in this case, fire perimeters), download data granules for:\n",
    "\n",
    "VIIRS/NPP Active Fires 6-Min L2 Swath 375m V002 (VNP14IMG)\n",
    "VIIRS/NPP Imagery Resolution Terrain Corrected Geolocation 6-Min L1 Swath 375 m (VNP03IMG)\n",
    "\n",
    "Return: \n",
    "    - Downloaded NetCDF granules for the above products\n",
    "    - GeoDataFrame representing active fire pixel locations and attributes (before geolocation)\n",
    "    - Geolocation grid representing pixel locations and overlap of adjacent orbits\n",
    "\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import math\n",
    "import contextlib\n",
    "import traceback\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "\n",
    "from netCDF4 import Dataset \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from affine import Affine\n",
    "from osgeo import gdal, gdal_array, gdalconst, osr\n",
    "from rasterio.transform import from_bounds\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Directories\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "# Output directories\n",
    "dataraw = os.path.join(projdir,'data/spatial/raw/VIIRS/')\n",
    "datamod = os.path.join(projdir,'data/spatial/mod/VIIRS/')\n",
    "\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc73e6e-f469-4785-91a3-a8b9660875b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lookup table for pixel sizes\n",
    "fp = os.path.join(projdir,'data/tabular/raw/pix_size_lut.csv')\n",
    "lut = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7ebd1-d1ba-400d-8b65-d3b993c033ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Download_VIIRS_AFD:\n",
    "    \"\"\" Downloads VIIRS Active Fire Data (AFD) within a region for given date range \"\"\"\n",
    "    def __init__(self, start_date, last_date, geom = gpd.GeoDataFrame(),\n",
    "                 fire_id_col = 'Fire_ID', fire_name_col = 'Fire_Name',\n",
    "                 region_extent = None, # broader region to process granules\n",
    "                 geog_crs = 'EPSG:4326', proj_crs = 'EPSG:5070',\n",
    "                 short_names = ['VNP14IMG', 'VNP03IMG'], # active fire data and associated geolocation\n",
    "                 buffer = None, out_directory=None, processed_granules=None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - start_date: the intial date for the granule search\n",
    "            - last_date: the final date for the granule search\n",
    "            - gdf: GeoDataFrame for search request\n",
    "            - geog_crs: Geographic projection (to retrieve coordinate pairs in lat/lon)\n",
    "            - proj_crs: Projected coordinate system\n",
    "            - short_names: the granules to be downloaded\n",
    "            - buffer: Optional buffer for input geometry\n",
    "        Returns:\n",
    "            - Downloaded files (VIIRS Active Fire Data NetCDF and Geolocation information)\n",
    "            - GeoDataFrame with non-geolocated (raw) fire detections\n",
    "        \"\"\"\n",
    "        # Extract coordinate bounds\n",
    "        self.coords, self.extent = get_coords(geom, buffer)\n",
    "        print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "\n",
    "        self.region = region_extent\n",
    "\n",
    "        self.fire_id = geom[fire_id_col].iloc[0]\n",
    "        self.fire_name = geom[fire_name_col].iloc[0]\n",
    "        \n",
    "        # Extract class attributes\n",
    "        self.date_range = (str(start_date), str(last_date))\n",
    "        self.geog_crs = geog_crs\n",
    "        self.proj_crs = proj_crs\n",
    "        self.short_names = short_names\n",
    "        self.out_dir = out_directory\n",
    "\n",
    "        self.processed_granules = processed_granules\n",
    "\n",
    "    \n",
    "    def ea_search_request(self):\n",
    "        \"\"\" Generate an earthaccess search request with the given parameters \"\"\"\n",
    "\n",
    "        query = earthaccess.search_data(\n",
    "            short_name=self.short_names, \n",
    "            polygon=self.coords,\n",
    "            temporal=self.date_range, \n",
    "        )\n",
    "\n",
    "        # Extract the query dataframe\n",
    "        df = pd.json_normalize(query)\n",
    "        # Grab a list of granule IDs\n",
    "        granules = list(df['meta.native-id'].unique())\n",
    "\n",
    "        # Filter the query to only download the \"new\" granules\n",
    "        new = [g for g in granules if g not in self.processed_granules]\n",
    "        query_ = [item for item in query if item['meta']['native-id'] in new]\n",
    "\n",
    "        # Download the \"new\" granules\n",
    "        earthaccess.download(query_, self.out_dir)\n",
    "        \n",
    "        # return fileset\n",
    "        return df, granules\n",
    "             \n",
    "\n",
    "    def create_fire_gdf(self):\n",
    "        \"\"\" Creates a geodataframe with active fire detections from a directory with NetCDF files \"\"\"\n",
    "\n",
    "        extent = self.region\n",
    "        \n",
    "        # Identify VNP14 vs. VNP03\n",
    "        vnp14_files = list_files(self.out_dir, \"VNP14IMG*.nc\", recursive=True)\n",
    "        vnp03_files = list_files(self.out_dir, \"VNP03IMG*.nc\", recursive=True)\n",
    "        \n",
    "        out_fire_dfs = []\n",
    "        \n",
    "        nprint = round(len(vnp14_files) / 4)\n",
    "        \n",
    "        for idx, fp in enumerate(sorted(vnp14_files)):\n",
    "            \n",
    "            # Gather some metadata information from the file name\n",
    "            timestamp = fp.split('.')[1:3]\n",
    "            year = timestamp[0][1:5]\n",
    "            day = timestamp[0][5:8]\n",
    "            time = timestamp[1]\n",
    "            date = dt.datetime.strptime(year+day, '%Y%j').strftime('%b %d') \n",
    "            acq_date = dt.datetime.strptime(year+day, '%Y%j').strftime('%-m/%-d/%y') #match FIRMS\n",
    "            daytime = int(time) > 1500 #timestamps in the 1900h-2200h UTC range are afternoon for Western US\n",
    "\n",
    "            # Find matching geolocation data file for one record\n",
    "            identifier = os.path.basename(fp)[:-3]\n",
    "            parts = identifier.split('.') # split by '.'\n",
    "            date_time_part = '.'.join(parts[1:4])  # Extract date-time parts for the VNP Version 002    \n",
    "            geo_id = 'VNP03IMG' + '.' + date_time_part\n",
    "            geo_da_fp = [geo_link for geo_link in vnp03_files if geo_id in os.path.basename(geo_link)][0] \n",
    "            \n",
    "            if geo_da_fp is None:\n",
    "                print(f\"!!! No geolocation file found for: {identifier}\")\n",
    "                continue\n",
    "            \n",
    "            # Read the geolocation data\n",
    "            with xr.open_dataset(geo_da_fp, engine='netcdf4', group='geolocation_data') as geo:\n",
    "                i, j = np.indices(geo.longitude.shape) #line and sample\n",
    "                # Crop to fire bounding extent\n",
    "                scene = ((geo.longitude > extent[0]) & (geo.longitude < extent[1]) & \n",
    "                         (geo.latitude > extent[2]) & (geo.latitude < extent[3])).values\n",
    "\n",
    "            # Get the VNP14IMG fire mask, etc\n",
    "            with xr.open_dataset(fp, engine='netcdf4') as vnp14:\n",
    "        \n",
    "                qa = vnp14['algorithm QA']\n",
    "                fire = vnp14['fire mask']\n",
    "                daynight = vnp14.DayNightFlag #string Day or Night\n",
    "                \n",
    "                lonfp = vnp14.variables['FP_longitude'][:] # fire pixel longitude\n",
    "                latfp = vnp14.variables['FP_latitude'][:]\n",
    "                frp = vnp14.variables['FP_power'][:] # fire radiative power\n",
    "\n",
    "                tree = cKDTree(np.array([lonfp, latfp]).T) #search tree for finding nearest FRP\n",
    "\n",
    "            # Set up a pandas dataframe for the swath\n",
    "            df = pd.DataFrame()\n",
    "            df['longitude'] = list(geo.longitude.values[scene])\n",
    "            df['latitude'] = list(geo.latitude.values[scene])\n",
    "            df['fire_mask'] = list(fire.values[scene])\n",
    "            df['daynight'] = daynight[0]\n",
    "            df['confidence'] = df.fire_mask\n",
    "            df.confidence = df.confidence.replace(\n",
    "                {0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "            df['acq_date'] = acq_date\n",
    "            df['acq_time'] = time\n",
    "            df['j'] = list(j[scene]) #sample number for pixel size lookup\n",
    "            \n",
    "            # Retain only low-high confidence fire points\n",
    "            df = df[df['fire_mask'] > 6]\n",
    "            df['fire_mask'] = pd.Categorical(df['fire_mask'])\n",
    "            known = df[df.confidence!='x'] # keep only low-high confidence fire pixels\n",
    "            \n",
    "            # gather frp\n",
    "            for k in known.index:\n",
    "                dist, nearest = tree.query([ known.loc[k, 'longitude'], known.loc[k, 'latitude'] ])\n",
    "                df.loc[k, 'frp'] = frp[nearest].item()\n",
    "        \n",
    "            # Join to pixel size info\n",
    "            df = pd.merge(df, lut, left_on='j', right_on='sample', how='left')\n",
    "            df.drop(columns=['j'], inplace=True)\n",
    "            out_fire_dfs.append(df)\n",
    "\n",
    "            df['granule'] = identifier # add the identifier as the granule\n",
    "        \n",
    "            if idx % nprint == 0:\n",
    "                print(f\"Processed {idx+1} observations.\")\n",
    "\n",
    "            # Remove the netcdf files from the system and clear space\n",
    "\n",
    "            del geo, vnp14\n",
    "            # os.remove(geo_da_fp)\n",
    "            # os.remove(fp)\n",
    "        \n",
    "        # Concatenate the out dfs\n",
    "        fire_data = pd.concat(out_fire_dfs) # for the entire list of granules\n",
    "        fire_data.to_csv(os.path.join(datamod,f'vnp14img_F{str(idx)}_{self.fire_id}_geo.csv'))\n",
    "        \n",
    "        return fire_data\n",
    "\n",
    "print(\"Class and functions ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e59e97-5dfa-4974-a474-60f7eb88a896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fda4e-77aa-449d-a1f6-a2539419007a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c184f-ae2d-4c6e-8d1f-1fdad2122e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extent around the Southern Rockies\n",
    "fp = os.path.join(projdir,'data/spatial/raw/boundaries/na_cec_eco_l3_west.gpkg')\n",
    "ecol3 = gpd.read_file(fp)\n",
    "srm = ecol3[ecol3['NA_L3NAME'] == 'Southern Rockies']\n",
    "print(srm.columns)\n",
    "\n",
    "buffer = 10000\n",
    "srm_coords, srm_extent = get_coords(srm, buffer)\n",
    "print(f\"Bounding Extent: \\n{srm_extent}\")\n",
    "\n",
    "# Unpack extent and create bounding box\n",
    "minx, maxx, miny, maxy = srm_extent\n",
    "bounding_box = box(minx, miny, maxx, maxy)\n",
    "bbox_gdf = gpd.GeoDataFrame(geometry=[bounding_box], crs=\"EPSG:4326\")  # Assuming WGS84 geographic coordinates\n",
    "\n",
    "# Plot the bounding box and SRM boundary\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Ensure SRM is in the correct CRS\n",
    "srm_ = srm.to_crs('EPSG:4326')\n",
    "srm_.plot(ax=ax, color=\"lightblue\", edgecolor=\"blue\", linewidth=1, label=\"SRM Boundary\")\n",
    "bbox_gdf.plot(ax=ax, color=\"none\", edgecolor=\"red\", linewidth=1.5, linestyle=\"--\", label=\"Bounding Box\")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "# Export bounding box to shapefile\n",
    "out_fp = os.path.join(projdir, \"data/spatial/mod/SRM_bounding_box.shp\")\n",
    "bbox_gdf.to_file(out_fp)\n",
    "print(f\"Bounding box saved to: {out_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ed65d-83b6-4892-8253-e4b4158beab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217be10-a50b-472b-ac0d-98a2ebc9069a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1db138-d9da-4606-a1fe-7154b40c17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fire dataset for the Southern Rockies\n",
    "fires_path = os.path.join(projdir,'data/spatial/mod/NIFC/nifc-ics_2018_to_2023-aspen_SRM.gpkg')\n",
    "fires = gpd.read_file(fires_path)\n",
    "fires.rename(columns={'NIFC_ID': 'Fire_ID', 'NIFC_NAME': 'Fire_Name'}, inplace=True)\n",
    "fires['DISCOVERY_DATE'] = fires['DISCOVERY_DATE'].dt.date\n",
    "fires['WF_CESSATION_DATE'] = fires['WF_CESSATION_DATE'].dt.date\n",
    "print(fires.columns)\n",
    "len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a413c62-4f7f-4396-a1e2-e45d83b943cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with \"large\" fires\n",
    "fires['NIFC_ACRES'] = fires['NIFC_ACRES'].astype(float)\n",
    "fires['NIFC_ACRES'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cc83b-232f-430b-a0b3-5f2a70de71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires = fires[fires['NIFC_ACRES'] > 1500]\n",
    "len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd27dcc-811b-4a02-a569-cbf7bb053d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8949b-5ad5-4c30-9da8-b096f854e90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c5c78-9481-497f-b583-f3d8ddf243f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72cfb9-1d17-423c-aa9a-46e310541a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 10000 \n",
    "srm_coords, srm_extent = get_coords(srm, buffer)\n",
    "print(f\"Bounding Extent: \\n{srm_extent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c3128-d09b-4523-a0e9-bf90a63b5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Get a list of fire IDs\n",
    "fire_ids = list(fires['Fire_ID'].unique())\n",
    "\n",
    "\n",
    "afd_dfs = [] # to store the output geodataframes\n",
    "granules_p = [] # to store processed granules\n",
    "for fire_id in fire_ids[0:3]:\n",
    "    t00 = time.time()\n",
    "\n",
    "    print(fire_id)\n",
    "\n",
    "    fire = fires[fires['Fire_ID'] == fire_id]\n",
    "    start_dte = fire['DISCOVERY_DATE'].iloc[0]\n",
    "    end_dte = fire['WF_CESSATION_DATE'].iloc[0]\n",
    "\n",
    "    print(f\"Processing for {fire['Fire_Name'].iloc[0]} Fire ...\\n\")\n",
    "    \n",
    "    # Initiate the download and extract class\n",
    "    downloader = Download_VIIRS_AFD(\n",
    "        start_date=start_dte,\n",
    "        last_date=end_dte,\n",
    "        geom=fire,\n",
    "        region_extent=srm_extent,\n",
    "        buffer=1000,\n",
    "        short_names=['VNP14IMG','VNP03IMG'],\n",
    "        out_directory=dataraw,\n",
    "        processed_granules=granules_p # already processed\n",
    "    )\n",
    "    \n",
    "    # Retrieve the search results\n",
    "    try:\n",
    "        # Download unique query\n",
    "        query_df, granules = downloader.ea_search_request()\n",
    "        granules_p.append(granules)\n",
    "        \n",
    "        # Create the active fire detection geodataframe\n",
    "        print(f\"\\n\\tGeolocating active fires ...\\n\")\n",
    "        afd_fire = downloader.create_fire_gdf()\n",
    "        afd_dfs.append(afd_fire_year)\n",
    "        \n",
    "        del afd_fire\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping fire {fire_id}\\n{e}\")\n",
    "        traceback.print_exc()  # This will print the full traceback\n",
    "        continue # continue to the next fire id\n",
    "\n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"Total elapsed time for {fire_year}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "# Concatenate the results and save out the geodataframe of latlon fire pixels (non-geolocated)\n",
    "afds = pd.concat(afd_dfs, ignore_index=True)\n",
    "afds.to_file(os.path.join(datamod,'vnp14img_SRM_aspen-fires_2018_to_2023_geo.gpkg'))\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t2:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a7a51-6f33-42d0-93b7-a1dac0f7405e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
