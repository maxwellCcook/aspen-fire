{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec03ae-2946-4baf-a5f7-f722a615f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloading VIIRS Active Fire Detections (AFD) with 'earthaccess' python API\n",
    "\n",
    "For a given geometry (in this case, fire perimeters), download data granules for:\n",
    "\n",
    "VIIRS/NPP Active Fires 6-Min L2 Swath 375m V002 (VNP14IMG)\n",
    "VIIRS/NPP Imagery Resolution Terrain Corrected Geolocation 6-Min L1 Swath 375 m (VNP03IMG)\n",
    "\n",
    "Return: \n",
    "    - Downloaded NetCDF granules for the above products\n",
    "    - GeoDataFrame representing active fire pixel locations and attributes (before geolocation)\n",
    "    - Geolocation grid representing pixel locations and overlap of adjacent orbits\n",
    "\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import math\n",
    "import contextlib\n",
    "import traceback\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "\n",
    "from netCDF4 import Dataset \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from affine import Affine\n",
    "from osgeo import gdal, gdal_array, gdalconst, osr\n",
    "from rasterio.transform import from_bounds\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "\n",
    "# Directories\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "# Output directories\n",
    "dataraw = os.path.join(projdir,'data/spatial/raw/VIIRS/')\n",
    "datamod = os.path.join(projdir,'data/spatial/mod/VIIRS/')\n",
    "\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b7ebd1-d1ba-400d-8b65-d3b993c033ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class and functions ready !\n"
     ]
    }
   ],
   "source": [
    "class Download_VIIRS_AFD:\n",
    "    \"\"\" Downloads VIIRS Active Fire Data (AFD) for a geometry \"\"\"\n",
    "    def __init__(self, start_date, last_date, gdf = gpd.GeoDataFrame(), \n",
    "                 geog_crs = 'EPSG:4326', proj_crs = 'EPSG:5070', id_col='Fire_ID', name_col='Fire_Name',\n",
    "                 short_names = ['VNP14IMG', 'VNP03IMG'], # active fire data and associated geolocation\n",
    "                 buffer = None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - start_date: the intial date for the granule search\n",
    "            - last_date: the final date for the granule search\n",
    "            - gdf: GeoDataFrame for search request\n",
    "            - geog_crs: Geographic projection (to retrieve coordinate pairs in lat/lon)\n",
    "            - id_col: unique identifier in the GeoDataFrame\n",
    "            - short_names: the granules to be downloaded\n",
    "        Returns:\n",
    "            - Downloaded files (VIIRS Active Fire Data NetCDF and Geolocation information)\n",
    "            - GeoDataFrame with non-geolocated (raw) fire detections\n",
    "        \"\"\"\n",
    "        \n",
    "        self.id = gdf[id_col].iloc[0] # grab the unique ID\n",
    "        self.fire_name = gdf[name_col].iloc[0] # fire name\n",
    "        self.crs = gdf.crs # the native CRS definition for the input geodataframe\n",
    "        self.geog_crs = geog_crs\n",
    "        self.proj_crs = proj_crs\n",
    "        if buffer is not None:\n",
    "            self.gdf = gdf\n",
    "            self.gdf = self.gdf.assign(geometry=self.gdf.buffer(buffer)) # buffer units in meters\n",
    "        else:\n",
    "            self.gdf = gdf\n",
    "        self.bounds = self.gdf.to_crs(geog_crs).unary_union.envelope # for bounds, coords ensure geographic projection\n",
    "        self.coords = list(self.bounds.exterior.coords)\n",
    "        self.short_names = short_names\n",
    "        self.out_dir = os.path.join(dataraw,f\"{self.fire_name}\")\n",
    "        self.date_range = (start_date, last_date)\n",
    "    \n",
    "    \n",
    "    def ea_search_request(self):\n",
    "        \"\"\" generate an earthaccess search request with the given parameters \"\"\"\n",
    "        print(f'Requesting data for: {self.fire_name} Fire')\n",
    "            \n",
    "        search_dict = {} # to store the search results\n",
    "        for short_name in self.short_names:\n",
    "            try:\n",
    "                # Search for products matching our short names\n",
    "                result = earthaccess.search_data(\n",
    "                    short_name=short_name,\n",
    "                    polygon=self.coords,\n",
    "                    temporal=self.date_range,\n",
    "                    count=1000, \n",
    "                )\n",
    "\n",
    "                # Check if there is valid data, if not, skip\n",
    "                if len(result) != 0:\n",
    "                    # Append the search results data frame to the dictionary\n",
    "                    search_dict[short_name] = result\n",
    "                else:\n",
    "                    raise ValueError(f'No data found for: {short_name} -- Polygon ID {self.id}')\n",
    "                                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping polygon ID {self.id}: {short_name}\")\n",
    "                continue\n",
    "\n",
    "        if not search_dict:\n",
    "            return None  # Return None for invalid search results\n",
    "        else:\n",
    "            return search_dict\n",
    "\n",
    "    \n",
    "    def download_results(self, search_dict):\n",
    "        \"\"\" Downloads the search results to directory \"\"\"\n",
    "        if search_dict is not None:\n",
    "            for key, result in search_dict.items():\n",
    "                # Set the output directory based on short_name\n",
    "                fd = os.path.join(self.out_dir, f'{key}/')\n",
    "                if not os.path.exists(fd):\n",
    "                    os.makedirs(fd)\n",
    "                if len(os.listdir(fd)) < len(result):\n",
    "                    # Download the the search results\n",
    "                    with open(os.devnull, 'w') as f, contextlib.redirect_stdout(f):\n",
    "                        earthaccess.download(result, local_path=fd)\n",
    "                else:\n",
    "                    print(\"Files already downloaded, skipping ! \")\n",
    "\n",
    "    \n",
    "    def create_fire_gdf(self, extent):\n",
    "        \"\"\" Creates a geodataframe with active fire detections from a directory with NetCDF files \"\"\"\n",
    "        \n",
    "        # List of downloaded .nc files\n",
    "        vnp14 = list_files(os.path.join(self.out_dir,'VNP14IMG'), \"*.nc\", recursive=True)\n",
    "        vnp03 = list_files(os.path.join(self.out_dir,'VNP03IMG'), \"*.nc\", recursive=True)\n",
    "\n",
    "        N = round(len(vnp14) / 4)\n",
    "        \n",
    "        out_fire_dfs = []\n",
    "        for idx, fp in enumerate(sorted(vnp14)):\n",
    "            \n",
    "            # Gather some metadata information from the file name\n",
    "            timestamp = fp.split('.')[1:3]\n",
    "            year = timestamp[0][1:5]\n",
    "            day = timestamp[0][5:8]\n",
    "            time = timestamp[1]\n",
    "            date = dt.datetime.strptime(year+day, '%Y%j').strftime('%b %d') \n",
    "            acq_date = dt.datetime.strptime(year+day, '%Y%j').strftime('%-m/%-d/%y') #match FIRMS\n",
    "            daytime = int(time) > 1500 #timestamps in the 1900h-2200h UTC range are afternoon for Western US\n",
    "\n",
    "            # Find matching geolocation data file for one record\n",
    "            identifier = os.path.basename(fp)[:-3]\n",
    "            parts = identifier.split('.') # split by '.'\n",
    "            date_time_part = '.'.join(parts[1:4])  # Extract date-time parts for the VNP Version 002    \n",
    "            geo_id = 'VNP03IMG' + '.' + date_time_part\n",
    "            geo_da_fp = [geo_link for geo_link in vnp03 if geo_id in os.path.basename(geo_link)][0] \n",
    "            \n",
    "            if geo_da_fp is None:\n",
    "                print(f\"!!! No geolocation file found for: {identifier}\")\n",
    "                continue\n",
    "                \n",
    "            # Read the geolocation data\n",
    "            geo = xr.open_dataset(geo_da_fp, engine='netcdf4', group='geolocation_data')\n",
    "            i, j = np.indices(geo.longitude.shape) #line and sample\n",
    "            # Crop to fire bounding extent\n",
    "            scene = ((geo.longitude > extent[0]) & (geo.longitude < extent[1]) & (geo.latitude > extent[2]) & (geo.latitude < extent[3])).values\n",
    "            \n",
    "            # Get the VNP14IMG fire mask, etc\n",
    "            vnp14 = xr.open_dataset(fp, engine='netcdf4')\n",
    "        \n",
    "            qa = vnp14['algorithm QA']\n",
    "            fire = vnp14['fire mask']\n",
    "            daynight = vnp14.DayNightFlag #string Day or Night\n",
    "            \n",
    "            lonfp = vnp14.variables['FP_longitude'][:] # fire pixel longitude\n",
    "            latfp = vnp14.variables['FP_latitude'][:]\n",
    "            frp = vnp14.variables['FP_power'][:] # fire radiative power\n",
    "        \n",
    "            tree = cKDTree(np.array([lonfp, latfp]).T) #search tree for finding nearest FRP\n",
    "        \n",
    "            # Set up a pandas dataframe for the swath\n",
    "            df = pd.DataFrame()\n",
    "            df['longitude'] = list(geo.longitude.values[scene])\n",
    "            df['latitude'] = list(geo.latitude.values[scene])\n",
    "            df['fire_mask'] = list(fire.values[scene])\n",
    "            df['daynight'] = daynight[0]\n",
    "            df['confidence'] = df.fire_mask\n",
    "            df.confidence = df.confidence.replace({0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "            df['acq_date'] = acq_date\n",
    "            df['acq_time'] = time\n",
    "            df['j'] = list(j[scene]) #sample number for pixel size lookup\n",
    "            \n",
    "            # Retain only low-high confidence fire points\n",
    "            df = df[df['fire_mask'] > 6]\n",
    "            df['fire_mask'] = pd.Categorical(df['fire_mask'])\n",
    "            known = df[df.confidence!='x'] # keep only low-high confidence fire pixels\n",
    "        \n",
    "            #gather frp\n",
    "            for k in known.index:\n",
    "                dist, nearest = tree.query([ known.loc[k, 'longitude'], known.loc[k, 'latitude'] ])\n",
    "                df.loc[k, 'frp'] = frp[nearest].item()\n",
    "        \n",
    "            # Join to pixel size info\n",
    "            df = pd.merge(df, lookup, left_on='j', right_on='sample', how='left')\n",
    "            df.drop(columns=['j'], inplace=True)\n",
    "            out_fire_dfs.append(df)\n",
    "        \n",
    "            if idx % N == 0:\n",
    "                print(f\"Processed {idx+1} observations.\")\n",
    "\n",
    "            # Clean up\n",
    "            del geo, scene, vnp14, qa, fire, daynight, lonfp, latfp, frp, tree, df\n",
    "            os.remove(fp)\n",
    "            os.remove(geo_da_fp)\n",
    "\n",
    "            gc.collect()\n",
    "    \n",
    "        # Concatenate the out dfs\n",
    "        fire_data = pd.concat(out_fire_dfs) # for the entire fire\n",
    "        fire_data['Fire_ID'] = self.id\n",
    "        fire_data['Fire_Name'] = self.fire_name\n",
    "        fire_data.to_csv(os.path.join(datamod,f'vnp14img_{self.fire_name.replace(\" \",\"_\")}_geo.csv'))\n",
    "        \n",
    "        return fire_data\n",
    "\n",
    "print(\"Class and functions ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c184f-ae2d-4c6e-8d1f-1fdad2122e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ed65d-83b6-4892-8253-e4b4158beab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217be10-a50b-472b-ac0d-98a2ebc9069a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1db138-d9da-4606-a1fe-7154b40c17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fire_ID', 'Fire_Name', 'NIFC_ACRES', 'FINAL_ACRES', 'pct_aspen',\n",
      "       'INCIDENT_ID', 'INCIDENT_NAME', 'START_YEAR', 'CAUSE', 'DISCOVERY_DATE',\n",
      "       'DISCOVERY_DOY', 'WF_CESSATION_DATE', 'WF_CESSATION_DOY',\n",
      "       'STR_DESTROYED_TOTAL', 'STR_DAMAGED_TOTAL', 'STR_THREATENED_MAX',\n",
      "       'EVACUATION_REPORTED', 'PEAK_EVACUATIONS', 'WF_PEAK_AERIAL',\n",
      "       'WF_PEAK_PERSONNEL', 'na_l3name', 'geometry'],\n",
      "      dtype='object')\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Load the fire dataset\n",
    "fires_path = os.path.join(projdir,'data/spatial/mod/NIFC/nifc-ics_2018_to_2023-aspen_SRM.gpkg')\n",
    "fires = gpd.read_file(fires_path)\n",
    "fires.rename(columns={'NIFC_ID': 'Fire_ID', 'NIFC_NAME': 'Fire_Name'}, inplace=True)\n",
    "print(fires.columns)\n",
    "print(len(fires))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cc83b-232f-430b-a0b3-5f2a70de71d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd27dcc-811b-4a02-a569-cbf7bb053d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extent around all fires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c463498c-da6f-4d43-96a9-edda0954e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample  along_scan  along_track  scan_angle  pix_area\n",
      "0       0    0.795616     0.783234     56.0600  0.623154\n",
      "1       1    0.794690     0.782908     56.0511  0.622169\n",
      "2       2    0.793765     0.782583     56.0422  0.621187\n",
      "3       3    0.792842     0.782258     56.0333  0.620207\n",
      "4       4    0.791921     0.781933     56.0244  0.619229\n"
     ]
    }
   ],
   "source": [
    "# Load the lookup table for pixel sizes\n",
    "fp = os.path.join(projdir,'data/tabular/raw/pix_size_lut.csv')\n",
    "lookup = pd.read_csv(fp)\n",
    "print(lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c3128-d09b-4523-a0e9-bf90a63b5bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Extent: \n",
      "[-108.0104287321153, -107.79428582032635, 37.37737010107216, 37.60899484292132]\n",
      "Requesting data for: 416 Fire\n",
      "Granules found: 94\n",
      "Granules found: 94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c2f61844544acf97652d85fcb2880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3248ee516fe84e4489e9021e898216bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33fcf9045054271b810f2f867b9cbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de77f2923a74946864108a171510fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154e73a2ea4c475a9b99f6b57c80d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Get a list of fire IDs\n",
    "fire_ids = fires['Fire_ID'].unique()\n",
    "\n",
    "afd_dfs = [] # to store the output geodataframes\n",
    "no_data_ids = [] # to store fire IDs with no data\n",
    "\n",
    "for fire_id in fire_ids:\n",
    "    t00 = time.time()\n",
    "    \n",
    "    fire = fires.loc[fires['Fire_ID'] == fire_id]\n",
    "    name = fire['Fire_Name']\n",
    "\n",
    "    # Grab an extent for cropping netcdf files\n",
    "    buffer = 1000 \n",
    "    coords, extent = get_coords(fire, buffer)\n",
    "    print(f\"Bounding Extent: \\n{extent}\")\n",
    "\n",
    "    # Initiate the download and extract class\n",
    "    downloader = Download_VIIRS_AFD(\n",
    "        gdf=fire,\n",
    "        start_date=fire['DISCOVERY_DATE'].iloc[0],\n",
    "        last_date=fire['WF_CESSATION_DATE'].iloc[0],\n",
    "        buffer=1000, # in meters\n",
    "    )\n",
    "    # Retrieve the search results\n",
    "    try:\n",
    "        search_results = downloader.ea_search_request()\n",
    "        if len(search_results) > 0:\n",
    "            # Downlaod the search results\n",
    "            downloader.download_results(search_results)\n",
    "            # Create the active fire detection geodataframe\n",
    "            print(f\"\\nCreating AFD geodataframe ...\\n\")\n",
    "            afd_fire = downloader.create_fire_gdf(extent)\n",
    "            afd_dfs.append(afd_fire)\n",
    "            del afd_fire\n",
    "        else:\n",
    "            raise ValueError(f'No data granules found for {fire_id}, skipping completely !')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping Fire ID {fire_id}\\n{e}\")\n",
    "        traceback.print_exc()  # This will print the full traceback\n",
    "        no_data_ids.append(fire_id)\n",
    "        break # continue to the next fire id\n",
    "\n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"Total elapsed time: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "# Concatenate the results and save out the geodataframe of latlon fire pixels (non-geolocated)\n",
    "afds = pd.concat(afd_dfs, ignore_index=True)\n",
    "afds.to_file(os.path.join(datamod,'vnp14img_aspen-fires_2018_to_2023_geo.gpkg'))\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t2:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
