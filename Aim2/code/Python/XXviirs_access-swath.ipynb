{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec03ae-2946-4baf-a5f7-f722a615f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloading VIIRS Active Fire Detections (AFD) with 'earthaccess' python API\n",
    "\n",
    "For a given geometry (in this case, fire perimeters), download data granules for:\n",
    "\n",
    "VIIRS/NPP Active Fires 6-Min L2 Swath 375m V002 (VNP14IMG)\n",
    "VIIRS/NPP Imagery Resolution Terrain Corrected Geolocation 6-Min L1 Swath 375 m (VNP03IMG)\n",
    "\n",
    "Return: \n",
    "    - Downloaded NetCDF granules for the above products\n",
    "    - GeoDataFrame representing active fire pixel locations and attributes (before geolocation)\n",
    "    - Geolocation grid representing pixel locations and overlap of adjacent orbits\n",
    "\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import math\n",
    "import contextlib\n",
    "import traceback\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import datetime\n",
    "\n",
    "from netCDF4 import Dataset \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from affine import Affine\n",
    "from osgeo import gdal, gdal_array, gdalconst, osr\n",
    "from rasterio.transform import from_bounds\n",
    "from scipy.spatial import cKDTree\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('earthaccess').setLevel(logging.ERROR)\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "        \n",
    "# Directories\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "# Output directories\n",
    "dataraw = os.path.join(projdir,'data/spatial/raw/VIIRS/')\n",
    "datamod = os.path.join(projdir,'data/spatial/mod/VIIRS/')\n",
    "\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73b7ebd1-d1ba-400d-8b65-d3b993c033ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class and functions ready !\n"
     ]
    }
   ],
   "source": [
    "class Access_VIIRS_AFD:\n",
    "    \"\"\" Accesses VIIRS Active Fire Data (AFD) within a region for given date range \"\"\"\n",
    "    def __init__(self, start_date, last_date, geom = gpd.GeoDataFrame(),\n",
    "                 id_col='Fire_ID', name_col='Fire_Name',\n",
    "                 geog_crs = 'EPSG:4326', proj_crs = 'EPSG:5070',\n",
    "                 short_names = ['VNP14IMG', 'VNP03IMG'], # active fire data and associated geolocation\n",
    "                 buffer = None, out_directory=None, processed_granules=None,\n",
    "                 download = False, region=None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - start_date: the intial date for the granule search\n",
    "            - last_date: the final date for the granule search\n",
    "            - geom: GeoDataFrame for search request (fire perimeter)\n",
    "            - geog_crs: Geographic projection (to retrieve coordinate pairs in lat/lon)\n",
    "            - proj_crs: Projected coordinate system\n",
    "            - short_names: the granules to be downloaded\n",
    "            - buffer: Optional buffer for input geometry\n",
    "            - out_directory: output directory to store results\n",
    "            - download: If 'True', downloads the netcdf, otherwise processes in cloud\n",
    "        Returns:\n",
    "            - Downloaded files (VIIRS Active Fire Data NetCDF and Geolocation information)\n",
    "            - GeoDataFrame with non-geolocated (raw) fire detections\n",
    "        \"\"\"\n",
    "        # Extract coordinate bounds\n",
    "        if region is None:\n",
    "            # use the fire perimeter\n",
    "            self.coords, self.extent = get_coords(geom, buffer)\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        elif region is not None and isinstance(region, gpd.GeoDataFrame):\n",
    "            # use the region boundary for FP and fire for search\n",
    "            _, self.extent = get_coords(region, buffer) # for extracting FP\n",
    "            self.coords, _ = get_coords(geom, buffer) # for data search\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        else:\n",
    "            print(\"Input region is not a GeoDataFrame !!!\")\n",
    "            \n",
    "        # Extract class attributes\n",
    "        self.fire_id = geom[id_col].iloc[0]\n",
    "        self.fire_name = geom[name_col].iloc[0]\n",
    "        self.date_range = (str(start_date), str(last_date))\n",
    "        self.geog_crs = geog_crs\n",
    "        self.proj_crs = proj_crs\n",
    "        self.short_names = short_names\n",
    "        self.out_dir = out_directory\n",
    "        self.download = download\n",
    "        self.processed_granules = processed_granules\n",
    "      \n",
    "    def ea_search_request(self):\n",
    "        \"\"\" Generate an earthaccess search request with the given parameters \"\"\"\n",
    "\n",
    "        query = earthaccess.search_data(\n",
    "            short_name=self.short_names, \n",
    "            polygon=self.coords,\n",
    "            temporal=self.date_range, \n",
    "            cloud_hosted=True,\n",
    "            count=-1\n",
    "        )\n",
    "        \n",
    "        # Grab a list of granule IDs (VNP14IMG)\n",
    "        granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier']\n",
    "                    for g in query if 'VNP14IMG' in g['umm']['DataGranule']['Identifiers'][0]['Identifier']]\n",
    "        N = len(granules)\n",
    "    \n",
    "        # Filter the query to only work with the \"new\" granules\n",
    "        # Skip if no new granules are required\n",
    "        if self.processed_granules is not None:\n",
    "            processed = [g.replace('.nc', '') for g in self.processed_granules]\n",
    "            new_granules = [g for g in granules if g not in processed]\n",
    "            if len(new_granules) == 0:\n",
    "                print(f\"\\n\\t! All granules already processed for region ({N - len(granules)}) !\\n\")\n",
    "                return None, None\n",
    "            elif len(new_granules) < N:\n",
    "                print(f\"\\n\\t! Some granules already processed for region ({N - len(granules)}) !\\n\")\n",
    "                # update the query\n",
    "                query = [item for item in query if item['umm']['DataGranule']['Identifiers'][0]['Identifier'] in new_granules\n",
    "                         or 'VNP03IMG' in item['umm']['DataGranule']['Identifiers'][0]['Identifier']]\n",
    "                granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier'] for g in query]\n",
    "            \n",
    "        if self.download is True and granules is not None:\n",
    "            # Download the \"new\" granules\n",
    "            earthaccess.download(query, self.out_dir)\n",
    "\n",
    "        # return query results and list of granules\n",
    "        return query, granules\n",
    "             \n",
    "\n",
    "    def create_fire_gdf(self, query):\n",
    "        \"\"\" Creates a geodataframe with active fire detections from a directory with NetCDF files \"\"\"\n",
    "\n",
    "        granule_dfs = [] # to store the geolocated AFDs\n",
    "        granule_log = os.path.join(datamod, 'logs/processed_granules.txt')\n",
    "        \n",
    "        # Identify VNP14 vs. VNP03\n",
    "        if self.download is True:\n",
    "            # Query the downloaded files\n",
    "            vnp14_files = list_files(os.path.join(self.out_dir,'VNP14IMG'), \"*.nc\", recursive=True)\n",
    "            vnp03_files = list_files(os.path.join(self.out_dir,'VNP03IMG'), \"*.nc\", recursive=True)\n",
    "        else:\n",
    "            vnp14_files = [g.data_links()[0] for g in query if 'VNP14IMG' in g.data_links()[0]]\n",
    "            vnp03_files = [g.data_links()[0] for g in query if 'VNP03IMG' in g.data_links()[0]]\n",
    "        \n",
    "        nprint = 10 # print counter\n",
    "        for idx, vnp14 in enumerate(sorted(vnp14_files)):\n",
    "\n",
    "            df = pd.DataFrame() # to store the active fire data\n",
    "                \n",
    "            # check if the granule has been processed\n",
    "            url = urlparse(vnp14)\n",
    "            granule_id = os.path.basename(url.path)    \n",
    "            if granule_id in self.processed_granules:\n",
    "                print(f\"\\t{granule_id} already processed. Skipping...\")\n",
    "\n",
    "            # gather information from file name\n",
    "            timestamp = granule_id.split('.')[1:3]\n",
    "            year = timestamp[0][1:5]\n",
    "            day = timestamp[0][5:8]\n",
    "            time = timestamp[1]\n",
    "            date = dt.datetime.strptime(year+day, '%Y%j').strftime('%b %d') \n",
    "            acq_date = dt.datetime.strptime(year+day, '%Y%j').strftime('%-m/%-d/%Y')\n",
    "            daytime = int(time) > 1500 #timestamps in the 1900h-2200h UTC range are afternoon for Western US\n",
    "            \n",
    "            # Identify the corresponding geolocation file\n",
    "            geo_id = 'VNP03IMG.' + \".\".join(timestamp)\n",
    "\n",
    "            if self.download is True:\n",
    "                # Grab the associated geolocation file\n",
    "                vnp03 = [g for g in vnp03_files if geo_id in os.path.basename(g)][0]\n",
    "            else:\n",
    "                # Filter the search query to the matching VNP14 and VNP03\n",
    "                query_ = [item for item in query if \".\".join(timestamp) in item.data_links()[0]]\n",
    "                # Open the VNP14IMG and gather the data\n",
    "                fileset = earthaccess.open(query_)  \n",
    "                vnp14 = fileset[1]\n",
    "                vnp03 = fileset[0]\n",
    "            \n",
    "            with xr.open_dataset(vnp14, phony_dims='access') as vnp14ds:\n",
    "\n",
    "                # Check for fire pixels in the specified region\n",
    "                lonfp = vnp14ds.variables['FP_longitude'][:] # fire pixel longitude\n",
    "                latfp = vnp14ds.variables['FP_latitude'][:] # fire pixel latitude\n",
    "                fire_scene = ((lonfp > self.extent[0]) & (lonfp < self.extent[1]) & \n",
    "                              (latfp > self.extent[2]) & (latfp < self.extent[3]))\n",
    "                if not fire_scene.any():  # Check for any fire pixels in region\n",
    "                    print(f\"\\n\\tNo active fires detected in {granule_id}. Skipping...\")\n",
    "                    with open(granule_log, 'a') as log_file:\n",
    "                        log_file.write(f\"{granule_id}\\n\") # log this granule as \"processed\"\n",
    "                    continue # skip if no fire pixels in region\n",
    "\n",
    "                # granule attributes\n",
    "                daynight = vnp14ds.DayNightFlag #string Day or Night\n",
    "                granule_id = vnp14ds.LocalGranuleID\n",
    "\n",
    "                # variables\n",
    "                fire = vnp14ds['fire mask'] # the fire mask\n",
    "                frp = vnp14ds.variables['FP_power'][:] # fire radiative power\n",
    "                t4 = vnp14ds.variables['FP_T4'][:] # I04 brightness temp (kelvins)\n",
    "                t5 = vnp14ds.variables['FP_T5'][:] # I05 brightness temp (kelvins)\n",
    "                \n",
    "                tree = cKDTree(np.array([lonfp, latfp]).T) #search tree for finding nearest FRP\n",
    "\n",
    "                del fire_scene\n",
    "                \n",
    "            # Read the geolocation data \n",
    "            with xr.open_dataset(vnp03, group='geolocation_data', phony_dims='access') as geo_ds:\n",
    "                i, j = np.indices(geo_ds.longitude.shape) #line and sample\n",
    "                # Crop to fire bounding extent\n",
    "                geo_scene = ((geo_ds.longitude > self.extent[0]) & (geo_ds.longitude < self.extent[1]) & \n",
    "                             (geo_ds.latitude > self.extent[2]) & (geo_ds.latitude < self.extent[3])).values\n",
    "            \n",
    "            # Populate the dataframe\n",
    "            df['longitude'] = list(geo_ds.longitude.values[geo_scene])\n",
    "            df['latitude'] = list(geo_ds.latitude.values[geo_scene])\n",
    "            df['fire_mask'] = list(fire.values[geo_scene])\n",
    "            df['confidence'] = pd.Categorical( df.fire_mask)\n",
    "            df.confidence = df.confidence.replace(\n",
    "                {0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "            df['daynight'] = daynight\n",
    "            df['acq_date'] = acq_date\n",
    "            df['acq_time'] = time\n",
    "            df['granule_id'] = granule_id\n",
    "            df['geo_id'] = geo_id\n",
    "            df['j'] = list(j[geo_scene]) #sample number for pixel size lookup\n",
    "            \n",
    "            # Retain only low-high confidence fire points\n",
    "            df = df[df.confidence!='x'] # keep only low-high confidence fire pixels\n",
    "        \n",
    "            # gather frp, brightness temps for nearest geolocated obs.\n",
    "            for k in df.index:\n",
    "                dist, nearest = tree.query([ df.loc[k, 'longitude'], df.loc[k, 'latitude'] ])\n",
    "                df.loc[k, 'frp'] = frp[nearest].item()\n",
    "                df.loc[k, 'iot4'] = t4[nearest].item()\n",
    "                df.loc[k, 'iot5'] = t5[nearest].item()\n",
    "        \n",
    "            # Join to pixel size info\n",
    "            df_ = pd.merge(df, lut, left_on='j', right_on='sample', how='left')\n",
    "            df_.drop(columns=['j'], inplace=True)\n",
    "            \n",
    "            granule_dfs.append(df_) # append the granule dataframe\n",
    "            \n",
    "            # clear up some memory and log the processed granule\n",
    "            del df, i, j, geo_scene, fire, latfp, lonfp, frp, tree, df_\n",
    "            if self.download is True:\n",
    "                os.remove(vnp14)\n",
    "                os.remove(vnp03)\n",
    "\n",
    "            with open(granule_log, 'a') as log_file:\n",
    "                log_file.write(f\"{granule_id}\\n\")\n",
    "\n",
    "            # write out the csv file\n",
    "            out_fp = os.path.join(datamod,f'granules/{granule_id.replace(\".\",\"_\")}.csv')\n",
    "            df_.to_csv(out_fp)\n",
    "\n",
    "            if idx % nprint == 0:\n",
    "                print(f\"\\n\\tProcessed {idx+1} granules.\\n\")\n",
    "\n",
    "        gc.collect() # clear out garbage\n",
    "        \n",
    "        # Concatenate the out dfs\n",
    "        if len(granule_dfs) > 0:\n",
    "            fire_data = pd.concat(granule_dfs) # for the entire list of granules\n",
    "            return fire_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "print(\"Class and functions ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fda4e-77aa-449d-a1f6-a2539419007a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ed65d-83b6-4892-8253-e4b4158beab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1db138-d9da-4606-a1fe-7154b40c17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available attributes: \n",
      "Index(['Fire_ID', 'Fire_Name', 'NIFC_ACRES', 'FINAL_ACRES', 'pct_aspen',\n",
      "       'INCIDENT_ID', 'INCIDENT_NAME', 'START_YEAR', 'CAUSE', 'DISCOVERY_DATE',\n",
      "       'DISCOVERY_DOY', 'WF_CESSATION_DATE', 'WF_CESSATION_DOY',\n",
      "       'STR_DESTROYED_TOTAL', 'STR_DAMAGED_TOTAL', 'STR_THREATENED_MAX',\n",
      "       'EVACUATION_REPORTED', 'PEAK_EVACUATIONS', 'WF_PEAK_AERIAL',\n",
      "       'WF_PEAK_PERSONNEL', 'na_l3name', 'geometry'],\n",
      "      dtype='object')\n",
      "\n",
      "There are [67] fires.\n"
     ]
    }
   ],
   "source": [
    "# Load the fire dataset for the Southern Rockies\n",
    "fp = os.path.join(projdir,'data/spatial/mod/NIFC/nifc-ics_2018_to_2023-aspen.gpkg')\n",
    "fires = gpd.read_file(fp)\n",
    "\n",
    "# subset to Southern Rockies\n",
    "fires = fires[fires['na_l3name'] == 'Southern Rockies']\n",
    "\n",
    "# tidy the fire id and name columns\n",
    "fires.rename(columns={'NIFC_ID': 'Fire_ID', 'NIFC_NAME': 'Fire_Name'}, inplace=True)\n",
    "\n",
    "# tify the date columns\n",
    "fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE']).dt.date\n",
    "fires['WF_CESSATION_DATE'] = pd.to_datetime(fires['WF_CESSATION_DATE']).dt.date\n",
    "\n",
    "# # Adjust the start and end dates\n",
    "# fires['start_date'] = fires['DISCOVERY_DATE'] - timedelta(days=2)\n",
    "# fires['end_date'] = fires['WF_CESSATION_DATE'] + timedelta(days=2)\n",
    "\n",
    "print(f\"Available attributes: \\n{fires.columns}\")\n",
    "print(f\"\\nThere are [{len(fires)}] fires.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9351c3e9-2129-4c63-85f3-5293a6fb2890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        67.000000\n",
       "mean      20818.361343\n",
       "std       54462.748925\n",
       "min          10.900000\n",
       "25%         210.860000\n",
       "50%        1497.900000\n",
       "75%       11361.310000\n",
       "max      299792.600000\n",
       "Name: NIFC_ACRES, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fires['NIFC_ACRES'] = fires['NIFC_ACRES'].astype(float)\n",
    "fires['NIFC_ACRES'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9bf0cb-8da6-4bc7-b69b-5ec7efde3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "['577' '416' 'LOADING PEN' 'PLATEAU' 'PLUMTAW' 'DOE CANYON'\n",
      " 'CHRIS MOUNTAIN' 'DRY LAKE' 'BEAR CREEK' 'QUARTZ RIDGE' 'TRAIL SPRINGS'\n",
      " 'MILL CREEK 2' 'THORPE' 'BUFFALO' 'LAKE CHRISTINE' 'SYLVAN'\n",
      " 'GRIZZLY CREEK' 'MIDDLE MAMM' 'PACK CREEK' 'OSHA' 'CALF CANYON'\n",
      " 'BLACK FEATHER' 'SARCA' 'CERRO PELADO' 'DECKER' 'MENKHAVEN'\n",
      " 'CAMERON PEAK' 'LEFTHAND' 'WILLIAMS FORK' 'TARANTULA' 'BULL DRAW'\n",
      " 'GREEN MOUNTAIN' 'POISON SPRINGS' 'COW CREEK' 'BRUSH CREEK' '403' 'AMOLE'\n",
      " 'SARDINAS CANYON' 'LUNA' 'GURULE' 'MONTOYA SPRINGS' 'CACHE CREEK' '441'\n",
      " 'WEST GUARD' 'BURRO' 'HORSE' 'ICE' 'SAND CREEK' 'EAST TROUBLESOME'\n",
      " 'MULLEN' 'MIDDLE FORK' 'MUDDY SLIDE' 'SUGARLOAF' 'MORGAN CREEK'\n",
      " 'BLACK MOUNTAIN' 'INDIAN RUN' 'SILVER CREEK' 'CABIN LAKE' 'REVEILLE'\n",
      " 'SPRING CREEK' 'WESTON PASS' 'BADGER CREEK' 'RYAN']\n"
     ]
    }
   ],
   "source": [
    "fires = fires[fires['NIFC_ACRES'] > 34.749]\n",
    "print(len(fires))\n",
    "print(fires['Fire_Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02611145-f7d9-44d4-b667-846909c8c218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>WF_CESSATION_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>2019-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2018-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>2020-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>2018-08-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-17</td>\n",
       "      <td>2022-05-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DISCOVERY_DATE WF_CESSATION_DATE\n",
       "1     2019-07-28        2019-08-18\n",
       "2     2018-06-01        2018-07-03\n",
       "3     2020-06-13        2020-06-18\n",
       "4     2018-07-22        2018-08-17\n",
       "5     2022-05-17        2022-05-18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fires[['DISCOVERY_DATE','WF_CESSATION_DATE']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8af0edb-153d-45e8-9d84-35086aa3215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2018-08-10    7\n",
      "2018-08-09    7\n",
      "2018-08-08    7\n",
      "2018-08-07    7\n",
      "2018-08-15    6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with individual dates for each fire\n",
    "date_counts = pd.DataFrame(\n",
    "    [(fire['Fire_ID'], single_date)\n",
    "     for _, fire in fires.iterrows()\n",
    "     for single_date in pd.date_range(fire['DISCOVERY_DATE'], fire['WF_CESSATION_DATE'])],\n",
    "    columns=['Fire_ID', 'Date']\n",
    ")['Date'].value_counts()\n",
    "print(date_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bb8949b-5ad5-4c30-9da8-b096f854e90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NA_L3CODE', 'NA_L3NAME', 'NA_L2CODE', 'NA_L2NAME', 'NA_L1CODE',\n",
      "       'NA_L1NAME', 'NA_L3KEY', 'NA_L2KEY', 'NA_L1KEY', 'Shape_Leng',\n",
      "       'Shape_Area', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the SRM bounds\n",
    "fp = os.path.join(projdir,'data/spatial/raw/boundaries/na_cec_eco_l3_west.gpkg')\n",
    "ecol3 = gpd.read_file(fp)\n",
    "srm = ecol3[ecol3['NA_L3NAME'] == 'Southern Rockies']\n",
    "print(srm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d56730-43fd-41fd-a9cd-f397360df7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cde3d6-58c3-4cfa-93d6-e89de42fe5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c92ac8f-4196-4e9f-a815-b7e45965d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for fire perimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c72c3128-d09b-4523-a0e9-bf90a63b5bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed [823] granules.\n",
      "Processing for OSHA fire:\n",
      "Bounding extent for data search: \n",
      "[-109.60932582670196, -103.84298510389097, 35.26644446378176, 42.79162551153811]\n",
      "\n",
      "Granules found: 4\n",
      "[]\n",
      "\n",
      "\t! All granules already processed for region (0) !\n",
      "\n",
      "Processing for 416 fire:\n",
      "Bounding extent for data search: \n",
      "[-109.60932582670196, -103.84298510389097, 35.26644446378176, 42.79162551153811]\n",
      "\n",
      "Granules found: 188\n",
      "[]\n",
      "\n",
      "\t! All granules already processed for region (0) !\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m~~~~~~~~~~\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Concatenate the results and save out the geodataframe of latlon fire pixels\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m afds \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafd_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mafds\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m afds\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datamod, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvnp14img_geo_aspen-fires_2018_to_2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "t0 = time.time()   \n",
    "\n",
    "# Check for already processed granules\n",
    "granule_log = os.path.join(datamod, 'logs/processed_granules.txt')\n",
    "if os.path.exists(granule_log):\n",
    "    with open(granule_log, 'r') as log_file:\n",
    "        granules_p = set([line.strip() for line in log_file.readlines()])\n",
    "else:\n",
    "    granules_p = []\n",
    "\n",
    "print(f\"Already processed [{len(granules_p)}] granules.\\n\")\n",
    "\n",
    "# Read in the already processed files, if it exists\n",
    "afds_p_fp = os.path.join(datamod, f'vnp14img_geo_aspen-fires-srm.csv')\n",
    "if len(granules_p) > 0:\n",
    "    afds_p = pd.read_csv(afds_p_fp)\n",
    "else:\n",
    "    afds_p = None\n",
    "                       \n",
    "# load the lookup table for pixel sizes\n",
    "lut = pd.read_csv(os.path.join(projdir,'data/tabular/raw/pix_size_lut.csv'))\n",
    "\n",
    "# Get a list of fire IDs sorted by ignition date\n",
    "fires = fires.sort_values(by=['START_YEAR','DISCOVERY_DATE'])\n",
    "fire_ids = fires['Fire_ID'].unique()\n",
    "\n",
    "afd_dfs = [] # to store the output geodataframes\n",
    "\n",
    "# Loop fire ids\n",
    "for fire_id in fire_ids:\n",
    "    t00 = time.time()\n",
    "\n",
    "    fire = fires[fires['Fire_ID'] == fire_id]\n",
    "    print(f\"Processing for {fire['Fire_Name'].iloc[0]} fire:\")\n",
    "    \n",
    "    da_access = Access_VIIRS_AFD(\n",
    "        start_date=fire['DISCOVERY_DATE'].iloc[0],\n",
    "        last_date=fire['WF_CESSATION_DATE'].iloc[0],\n",
    "        geom=fire,\n",
    "        buffer=1000,\n",
    "        short_names=['VNP14IMG','VNP03IMG'],\n",
    "        out_directory=dataraw,\n",
    "        processed_granules=granules_p,\n",
    "        download=False,\n",
    "        region=srm\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        query, granules = da_access.ea_search_request()\n",
    "\n",
    "        if granules is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n\\tGeolocating active fires ...\\n\")\n",
    "        afd_fire = da_access.create_fire_gdf(query)\n",
    "\n",
    "        # save the progress so far\n",
    "        if afd_fire is not None:\n",
    "            afd_dfs.append(afd_fire)\n",
    "            granules_p.extend(granules) # running list\n",
    "            # save out the fire data so far\n",
    "            if afds_p is not None:\n",
    "                pd.concat([afds_p, afds_fire]).to_csv(afds_p_fp)\n",
    "            del afd_fire\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping fire id {fire_id}\\n{e}\")\n",
    "        traceback.print_exc()  # This will print the full traceback\n",
    "        continue # continue to the next fire id\n",
    "\n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"\\nTotal elapsed time for {fire['Fire_Name']}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "# Concatenate the results and save out the geodataframe of latlon fire pixels\n",
    "afds = pd.concat(afd_dfs, ignore_index=True)\n",
    "print(f\"\\n{afds.head()}\\n\")\n",
    "afds.to_csv(os.path.join(datamod, f'vnp14img_geo_aspen-fires_2018_to_2023.csv'))\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"\\nTotal elapsed time: {t2:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a7a51-6f33-42d0-93b7-a1dac0f7405e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40956b-39d5-48b3-bbd7-2334286cce9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
