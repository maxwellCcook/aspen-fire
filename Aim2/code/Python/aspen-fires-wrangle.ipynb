{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce966a4-8d04-4c48-b714-cbbc9d534856",
   "metadata": {},
   "source": [
    "# Gathering data for wildfire which burned >= 5% aspen forest cover\n",
    "\n",
    "#### Fire data\n",
    "\n",
    "Fire data is gathered from the FIRED database between 2018-2023. From these fire perimeters, we calculate the percent aspen forest cover pre-burn to identify a set of wildfires which burned through aspen forests (at least 5% of pre-fire burned area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d797da2-4f14-4307-bafa-ef51add6105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Identifies a subset of wildfire perimeters which burned through >= 5% pre-fire aspen forest cover\n",
    "\n",
    "Workflow:\n",
    "    - Gather wildfire perimeters from FIRED (2018-2023) for the western US\n",
    "    - Calculate the LANDFIRE EVT (ca. 2016) categorical percentages\n",
    "    - Filter for wildfires where aspen cover >= 5%\n",
    "\n",
    "maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "proj = 'EPSG:5070'\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9a7b5-ce88-4e9c-af46-a0cc048d75c1",
   "metadata": {},
   "source": [
    "## Wrangle the Fire Data\n",
    "\n",
    "First, we need to identify wildfire events across the western U.S. which had at least 5% pre-fire aspen cover. We start with a database of FIRED events extracted to western US states. We then calculate the zonal histogram from the LANDFIRE EVT (ca. 2016 remap). From there, we can identify wildfires with >5% aspen cover.\n",
    "\n",
    "#### Calculate LANDFIRE EVT cover type (%) for each fire\n",
    "\n",
    "To start with, we will calculate the % cover for different land cover types within fire perimeters using FIRED and LANDFIRE EVT (ca. 2016 remap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d3aa6-c59b-4cb0-a4c5-a8146b306f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FIRED perimeters (2018-2022)\n",
    "fired_path = os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_events_west.gpkg')\n",
    "fired = gpd.read_file(fired_path).to_crs(proj)\n",
    "fired['ig_year'] = fired['ig_year'].astype(int)\n",
    "fired = fired[(fired['ig_year'] >= 2018) & (fired['ig_year'] <= 2023)]\n",
    "\n",
    "# Tidy some of the columns\n",
    "fired = fired.rename(columns={'id': 'fired_id'})\n",
    "fired['fired_id'] = fired['fired_id'].astype(str)\n",
    "fired['ig_year'] = fired['ig_year'].astype(int)\n",
    "\n",
    "print(f\"Start year: {fired['ig_year'].min()}, end year: {fired['ig_year'].max()}\")\n",
    "print(f\"There are {len(fired['fired_id'].unique())} unique fire events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86efef-a859-4ff6-abfb-a2758c4c02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LANDFIRE EVT (ca. 2016)\n",
    "evt_path = os.path.join(maindir,'data/landcover/LANDFIRE/LF2016_EVT_200_CONUS/Tif/LC16_EVT_200.tif')\n",
    "evt = rxr.open_rasterio(evt_path, masked=True).squeeze()\n",
    "shp, gt, wkt, nd = evt.shape, evt.spatial_ref.GeoTransform, evt.rio.crs, evt.rio.nodata\n",
    "print(\n",
    "    f\"Shape: {shp}; \\n\"\n",
    "    f\"GeoTransform: {gt}; \\n\"\n",
    "    f\"WKT: {wkt}; \\n\"\n",
    "    f\"NoData Value: {nd}; \\n\"\n",
    "    f\"Data Type: {evt[0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf556b-a298-4834-b203-86cedad952f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent cover from the LANDFIRE EVT\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Run the zonal statistics function (categorical raster data)\n",
    "zs = zonal_stats(vectors=fired[['fired_id','geometry']], raster=evt_path, categorical=True, geojson_out=True)\n",
    "\n",
    "# Extract the results as a geodataframe\n",
    "stats = gpd.GeoDataFrame(zs).fillna(0) #One column per raster category, and pixel count as value\n",
    "\n",
    "print(stats.head())\n",
    "print(len(stats['id'].unique()))\n",
    "\n",
    "del zs, evt, shp, gt, wkt, nd\n",
    "\n",
    "print(f\"Total elapsed time: {round((time.time() - t0))} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd67f2-4d3b-4d53-a400-e147d16ce591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the properties json\n",
    "\n",
    "# First get the fired_id\n",
    "stats['fired_id'] = stats['properties'].apply(lambda x: x.get('fired_id'))\n",
    "# Retrieve the other properties (EVT codes)\n",
    "stats['properties'] = stats['properties'].apply(lambda x: {key: val for key, val in x.items() if key != 'fired_id'})\n",
    "stats['properties_list'] = stats['properties'].apply(lambda x: list(x.items()))\n",
    "\n",
    "# Explode the json object\n",
    "props = stats.explode('properties_list').reset_index(drop=True)\n",
    "# retrieve the list items as new columns\n",
    "props[['EVT', 'pixel_count']] = pd.DataFrame(props['properties_list'].tolist(), index=props.index)\n",
    "props = props[['fired_id','EVT','pixel_count']].reset_index(drop=True)\n",
    "\n",
    "# Calculate the percent of each class\n",
    "total_pixels = props.groupby(props['fired_id'])['pixel_count'].transform('sum')\n",
    "props['total_pixels'] = total_pixels\n",
    "props['pct_cover'] = (props['pixel_count'] / props['total_pixels']) * 100\n",
    "\n",
    "print(props.head())\n",
    "print(props.columns)\n",
    "print(len(props['fired_id'].unique()))\n",
    "\n",
    "del stats, total_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983c164-e6cd-4a07-8f47-01cc801e5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lookup table for the EVT codes\n",
    "lookup = os.path.join(maindir,'data/landcover/LANDFIRE/LF2016_EVT_200_CONUS/CSV_Data/LF16_EVT_200.csv')\n",
    "lookup = pd.read_csv(lookup)\n",
    "print(lookup.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457a4ca-8f8b-45ec-ad43-a4b13fec6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the codes we want to join, join back to the dataframe\n",
    "lookup = lookup[['VALUE','EVT_NAME','EVT_PHYS','EVT_GP_N','EVT_CLASS']]\n",
    "# Merge back to the data\n",
    "props_df = props.merge(lookup, left_on='EVT', right_on='VALUE', how='left')\n",
    "\n",
    "print(props_df.head())\n",
    "print(len(props_df['fired_id'].unique()))\n",
    "\n",
    "del lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b35a3-30e9-442f-9349-1d46b1867d29",
   "metadata": {},
   "source": [
    "### Identify fires with at least 5% aspen forest cover\n",
    "\n",
    "Now that we have a data frame of landcover types for each wildfire, we can isolate the quaking aspen cover types and identify wildfires which had at least 5% pre-fire aspen forest cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc99ea-1324-48c7-b6ae-86d08ab0efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classes with 'aspen' in the EVT_NAME\n",
    "aspen_evt = props_df[props_df['EVT_NAME'].str.contains('aspen', case=False, na=False)]['EVT_NAME'].unique()\n",
    "print(aspen_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72cb6e-5d33-4718-b74e-a464714e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter to retain wildfire events with at least 5% aspen forest cover\n",
    "props_filtered = props_df[props_df['EVT_NAME'].isin(aspen_evt)]\n",
    "aspen_sum = props_filtered.groupby('fired_id')['pct_cover'].sum().reset_index() # get the sum of aspen classes\n",
    "aspen_sum['pct_aspen'] = aspen_sum['pct_cover']  # rename the column to retain aspen percent\n",
    "\n",
    "# Filter out fires with less than 5% aspen cover\n",
    "aspen_fires = aspen_sum[aspen_sum['pct_aspen'] >= 5] # retain fires with >= 5% \n",
    "aspen_fires = aspen_fires[['fired_id','pct_aspen']] # subset columns\n",
    "print(aspen_fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724fc10-3542-450b-af07-484ecc16f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back to the FIRED data\n",
    "# Check for matching fired_id set\n",
    "common_ids = set(fired['fired_id']).intersection(set(aspen_fires['fired_id'])) # Find intersection\n",
    "print(f\"Number of common IDs: {len(common_ids)}\")\n",
    "\n",
    "# Join the attribute data to FIRED polygons\n",
    "fired['fired_id'] = fired['fired_id'].astype(str)\n",
    "aspen_fires['fired_id'] = aspen_fires['fired_id'].astype(str)\n",
    "\n",
    "# Join aspen percent to the FIRED data\n",
    "fired_aspen = pd.merge(fired, aspen_fires, on='fired_id', how='inner')\n",
    "print(fired_aspen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec8e89-cd48-4c0a-8749-0eff2918446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fires that are only one MODIS pixel\n",
    "fired_aspen = fired_aspen[fired_aspen['tot_pix'] > 1]\n",
    "print(fired_aspen['tot_pix'].describe())\n",
    "# Filter out any \"cropland\" fires\n",
    "fired_aspen = fired_aspen[fired_aspen['lc_name'] != 'Croplands']\n",
    "print(fired_aspen['lc_name'].unique())\n",
    "\n",
    "print(f'There are {len(fired_aspen)} fire events meeting our criteria so far.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30419d9c-84d1-4da9-911b-ab6cc120ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this file out\n",
    "fired_aspen = fired_aspen.to_crs(proj)  # ensure the correct projection before exporting\n",
    "fired_aspen.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_events_west_aspen_all_gt5pct.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99305123-b1c3-4a01-8430-f5e6c0fce738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial map of aspen wildfires (centroid)\n",
    "\n",
    "# Load the state boundaries\n",
    "states = gpd.read_file(os.path.join(maindir,'data/boundaries/political/TIGER/tl19_us_states_west_nad83.gpkg'))\n",
    "\n",
    "# Generate centroids\n",
    "centroid = fired_aspen.copy()\n",
    "centroid['geometry'] = centroid.geometry.centroid\n",
    "\n",
    "# Make a spatial map of the centroids now\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "states.plot(ax=ax, edgecolor='black', linewidth=1, color='none')\n",
    "\n",
    "# Plot centroids\n",
    "centroid['size'] = centroid['pct_aspen'] * 10  # Adjust the scaling factor as necessary\n",
    "centroid.plot(\n",
    "    ax=ax, markersize=centroid['pct_aspen'], \n",
    "    column='pct_aspen', cmap='viridis', \n",
    "    legend=True, alpha=0.6, \n",
    "    legend_kwds={'label': \"Aspen Percent\"})\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "\n",
    "del centroid\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(projdir, 'figures/Fig1_all_aspen_fires.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95a47c-1c54-488c-8e56-9524bcf30666",
   "metadata": {},
   "source": [
    "#### Extract the daily fire perimeters for the aspen wildfire subset\n",
    "\n",
    "We need to also extract the daily fire perimeters from FIRED so we can perform further analysis on the temporal patterns of fire growth as it relates to aspen cover, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f8523-d1f4-466e-9b20-068f6cfc8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of IDs\n",
    "ids = fired_aspen['fired_id'].unique()\n",
    "\n",
    "# Load the daily polygons, subset to aspen fires\n",
    "daily_path = os.path.join(maindir,'FIRED/data/spatial/raw/events/events_040324/shapefiles/fired_conus_ak_2000_to_2024_daily.gpkg')\n",
    "daily = gpd.read_file(daily_path)\n",
    "daily['id'] = daily['id'].astype(str)\n",
    "daily = daily[daily['id'].isin(ids)]\n",
    "print(len(daily['id'].unique()))\n",
    "\n",
    "# Save out\n",
    "daily = daily.to_crs(proj)  # ensure the correct projection before exporting\n",
    "daily.to_file(os.path.join(projdir,'data/spatial/mod/FIRED/fired_daily_west_aspen_all_gt5pct.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed7417-3d84-41b8-96b7-2378b645f9e8",
   "metadata": {},
   "source": [
    "## Wrangle the MODIS 1km and VIIRS 375m Active Fire Detections (AFD)\n",
    "\n",
    "We downloaded the archive AFD for MODIS Collection 6.1 (1km), the Suomi National Polar-Orbiting Partnership (VIIRS S-NPP 375m) and NOAA-20 (VIIRS NOAA-20 375m) data products from the NASA FIRMS (https://firms.modaps.eosdis.nasa.gov/download/) between 2018-2023 in the western US. \n",
    "\n",
    "The VIIRS observations are split into archive (2018-2022) and \"NRT\" (2022-2023). These files need to be merged prior to performing the tidying.\n",
    "\n",
    "To start with, we will create a tidy database of VIIRS observations for both the S-NPP and NOAA-20 vintages. Then, we will look at creating a combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6fade-c645-48c0-a9ff-b3d132d0149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Set up the file paths\n",
    "\n",
    "modis = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_M-C61_476781/')\n",
    "snpp = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_SV-C2_476784/')\n",
    "noaa = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_J1V-C2_476782/')\n",
    "\n",
    "# Store these in a dictionary\n",
    "dict = {\n",
    "    \"MOD61\": modis,\n",
    "    \"SNPP\": snpp,\n",
    "    \"NOAA-20\": noaa\n",
    "}\n",
    "\n",
    "# Buffer fire perimeters for the extraction\n",
    "fire_buffer = fired_aspen.copy().to_crs(proj)\n",
    "fire_buffer['geometry'] = fire_buffer.geometry.buffer(1000)  # 1km buffer\n",
    "fire_buffer = fire_buffer[['fired_id','ig_date','last_date','geometry']] \n",
    "\n",
    "# Process each of the archive data products\n",
    "gdfs = {} # dictionary to store the geo data frames\n",
    "for key, path in dict.items():\n",
    "    print(f'Processing: {key}')\n",
    "    # Read in the archive vector data\n",
    "    vect = glob.glob(path+\"*.shp\")\n",
    "    print([os.path.basename(v) for v in vect])\n",
    "    if len(vect) > 1:\n",
    "        print(\"~Merging archive and NRT data.\")\n",
    "        archive = gpd.read_file([v for v in vect if \"archive\" in v][0]).to_crs(proj)\n",
    "        nrt = gpd.read_file([v for v in vect if \"nrt\" in v][0]).to_crs(proj)\n",
    "        # Concatenate the archive and NRT\n",
    "        afd = pd.concat([archive, nrt], ignore_index=True)\n",
    "        del archive, nrt\n",
    "    else:\n",
    "        afd = gpd.read_file(vect[0]).to_crs(proj)\n",
    "\n",
    "    # Add some attribute information\n",
    "    afd['VID'] = afd.index # unique ID column\n",
    "\n",
    "    # Remove low-confidence observations\n",
    "    try:\n",
    "        afd = afd[afd['CONFIDENCE'] != 'l']\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "\n",
    "    del vect\n",
    "\n",
    "    #################################################\n",
    "    # Extract AFDs within wildfire data (aspen fires)\n",
    "    \n",
    "    # Extract AFDs\n",
    "    afd_aspen = gpd.sjoin(afd, fire_buffer, how='inner', predicate='within')\n",
    "    print(afd_aspen.columns)\n",
    "    \n",
    "    del afd\n",
    "    \n",
    "    ####################################################\n",
    "    # Perform temporal filtering to remove false-positive matches\n",
    "\n",
    "    # First, create date columns\n",
    "    afd_aspen['ACQ_DATE'] = pd.to_datetime(afd_aspen['ACQ_DATE'])\n",
    "    afd_aspen['ACQ_MONTH'] = afd_aspen['ACQ_DATE'].dt.month.astype(int)\n",
    "    afd_aspen['ACQ_YEAR'] = afd_aspen['ACQ_DATE'].dt.year.astype(int)\n",
    "    afd_aspen['ig_date'] = pd.to_datetime(afd_aspen['ig_date'])\n",
    "    afd_aspen['last_date'] = pd.to_datetime(afd_aspen['last_date'])\n",
    "\n",
    "    # Filter based on ignition month and year\n",
    "    afd_aspen_f = afd_aspen[\n",
    "        (afd_aspen['ACQ_YEAR'] >= afd_aspen['ig_date'].dt.year.astype(int)) & \n",
    "        (afd_aspen['ACQ_MONTH'] >= afd_aspen['ig_date'].dt.month.astype(int)) &\n",
    "        (afd_aspen['ACQ_YEAR'] <= afd_aspen['last_date'].dt.year.astype(int)) &\n",
    "        (afd_aspen['ACQ_MONTH'] <= afd_aspen['last_date'].dt.month.astype(int))\n",
    "    ]\n",
    "    \n",
    "    # Keep unique rows\n",
    "    afd_aspen_f = afd_aspen_f.drop_duplicates(subset='VID', keep='first')    \n",
    "\n",
    "    del afd_aspen\n",
    "    \n",
    "    #############################################\n",
    "    # Remove fires with less than 10 observations\n",
    "\n",
    "    # Get a count per fire\n",
    "    afd_counts = afd_aspen_f.groupby('fired_id').size().reset_index(name='counts')\n",
    "\n",
    "    # Get a list of IDs of fires with > 1 obs.\n",
    "    ids = afd_counts[afd_counts[\"counts\"] > 1]\n",
    "    \n",
    "    # Grab the new list of unique FIRED ids\n",
    "    ids = ids['fired_id'].unique().tolist()\n",
    "    \n",
    "    # Filter the datasets based on these FIRED ids\n",
    "    afd_aspen_f = afd_aspen_f[afd_aspen_f['fired_id'].isin(ids)]\n",
    "    fired_aspen_f = fired_aspen[fired_aspen['fired_id'].isin(ids)]\n",
    "\n",
    "    print(f\"Minimum obs./fire: {afd_counts['counts'].min()}; \\nMaximum obs./fire: {afd_counts['counts'].max()}\")\n",
    "    print(f\"Number of fires after filtering: {len(fired_aspen_f)}\")\n",
    "    print(f\"Number of obs. after filtering: {len(afd_aspen_f)}\")\n",
    "\n",
    "    del afd_counts, ids\n",
    "    \n",
    "    #################################################\n",
    "    # Plot the distribution of observations over time\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    afd_aspen_f['ACQ_DATE'].hist(bins=100)\n",
    "    plt.title(f'{key} - AFDs (2018-2023)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Observations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    ##############################\n",
    "    # Append to the new dictionary\n",
    "    gdfs[key] = afd_aspen_f\n",
    "\n",
    "print(f\"Total elapsed time: {round((time.time() - t0))} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b147fcb-77cc-409f-8d9b-174e66c46079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the files as they are currently\n",
    "gdfs['MOD61'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/mod61_archive_afd_aspen.gpkg'))\n",
    "gdfs['SNPP'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/snpp_archive_afd_aspen.gpkg'))\n",
    "gdfs['NOAA-20'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/noaa20_archive_afd_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110adb8e-2c58-43c0-9238-3b23a36295a8",
   "metadata": {},
   "source": [
    "## Handling acquisition time-of-day and spatially overlapping observations\n",
    "\n",
    "The VIIRS S-NPP AFD have many overlapping observations during a single fire event. In some cases, the overlapping points are on the same day and time but with different FRP values. In these cases, it may be best to \n",
    "\n",
    "From here on we can work with just the SNPP because it has the most consistency across our time period (NOAA-20 started in 2020). Later we can investigate the combination of the three datasets or at least make a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740dcace-364d-4410-87c3-3729183d9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the SNPP observations\n",
    "\n",
    "snpp_aspen = gdfs['SNPP']\n",
    "\n",
    "# Filter out FRP == 0\n",
    "snpp_aspen = snpp_aspen[snpp_aspen['FRP'] > 0]\n",
    "snpp_aspen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58414-b092-4c75-8ac2-d3c60de9fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snpp_aspen['FRP'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698273-0922-4691-95cd-04d69c2a9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snpp_aspen['ACQ_TIME'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef843b08-b195-4b5f-811c-4743951e3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snpp_aspen = snpp_aspen.reset_index()\n",
    "snpp_aspen = snpp_aspen.rename(columns={'index':'index_'})\n",
    "snpp_aspen = snpp_aspen.drop(['index_right'], axis=1)\n",
    "print(snpp_aspen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba61900-2a4d-40cb-9e30-a206f07ad50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime object as a new column (in UTC)\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Function to convert ACQ_DATE and ACQ_TIME to a datetime object in UTC\n",
    "def convert_to_datetime(acq_date, acq_time):\n",
    "    # Ensure ACQ_TIME is in HHMM format\n",
    "    if len(acq_time) == 3:\n",
    "        acq_time = '0' + acq_time\n",
    "    elif len(acq_time) == 2:\n",
    "        acq_time = '00' + acq_time\n",
    "    elif len(acq_time) == 1:\n",
    "        acq_time = '000' + acq_time\n",
    "\n",
    "    acq_date_str = acq_date.strftime('%Y-%m-%d')\n",
    "    dt = datetime.strptime(acq_date_str + acq_time, '%Y-%m-%d%H%M')\n",
    "    dt_utc = pytz.utc.localize(dt)  # Localize the datetime object to UTC\n",
    "    return dt_utc\n",
    "\n",
    "# Apply the conversion function to create a new datetime column\n",
    "snpp_aspen.loc[:, 'ACQ_DATETIME'] = snpp_aspen.apply(lambda row: convert_to_datetime(row['ACQ_DATE'], row['ACQ_TIME']), axis=1)\n",
    "\n",
    "# Print the resulting GeoDataFrame with timezone-aware datetime objects\n",
    "print(snpp_aspen['ACQ_DATETIME'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da4785-d857-4fb3-9021-fb0b2dcae55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial intersection to identify overlapping observations\n",
    "overlap = gpd.sjoin(snpp_aspen, snpp_aspen, predicate='intersects', lsuffix='left', rsuffix='right')\n",
    "print(overlap.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df74c78-53a7-46f6-bbbe-fe7333ac18ef",
   "metadata": {},
   "source": [
    "#### Case 1: Same day/time observations with different FRP values (spatially overlapping)\n",
    "\n",
    "In this case, we have overlapping observations which have the same datetime but (sometimes) different FRP values. To handle this, we can group observations by datetime and perform a dissolve, taking the mean FRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078856f-4607-4b00-b377-3534292e1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique group ID for each set of intersecting observations with the same DATETIME\n",
    "overlap['_VID_'] = overlap.groupby(['ACQ_DATETIME_left', 'ACQ_DATETIME_right']).ngroup()\n",
    "print(overlap[['ACQ_DATETIME_left', 'ACQ_DATETIME_right','_VID_']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f164f-d9c4-4fd9-9f9f-28e8797b74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 90th percentile of VPD among the observations\n",
    "\n",
    "# Join the new _VID_ back to the original dataframe using VID\n",
    "snpp_aspen_ = snpp_aspen.merge(\n",
    "    overlap[['VID_left', '_VID_']].drop_duplicates(), \n",
    "    left_on='VID', right_on='VID_left', how='left').reset_index(drop=True)\n",
    "\n",
    "# Calculate the 90th percentile FRP for each _VID_\n",
    "def pct90(group):\n",
    "    group['FRP_90'] = np.percentile(group['FRP'], 90)\n",
    "    return group\n",
    "\n",
    "# Apply the function to calculate the 90th percentile FRP\n",
    "snpp_aspen_ = snpp_aspen_.groupby('_VID_').apply(pct90).reset_index(drop=True)\n",
    "snpp_aspen_[['_VID_','VID','ACQ_DATETIME','LATITUDE','LONGITUDE','FRP','FRP_90','VERSION']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7470e4-6ad2-41b2-a085-dc3e9fc309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve by the same day/time VID to create a new geometry with the 90th percentile FRP\n",
    "\n",
    "snpp_aspen_dis = snpp_aspen_.dissolve(by='_VID_').reset_index() # this takes the first of each, which should be OK\n",
    "\n",
    "snpp_aspen_dis.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdee6ea-fb9b-4df6-a0c6-9b5fa9a2f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out a version of these data\n",
    "\n",
    "# Create the buffered VIIRS obs.\n",
    "snpp_aspen_plot = snpp_aspen_dis.copy()\n",
    "snpp_aspen_plot['geometry'] = snpp_aspen_plot.geometry.buffer(375, cap_style=3)  # square buffer 375m\n",
    "\n",
    "# Save the VIIRS observations (points)\n",
    "snpp_aspen_dis = snpp_aspen_dis.to_crs(proj)\n",
    "snpp_aspen_dis.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_snpp_pt_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (plots)\n",
    "snpp_aspen_plot = snpp_aspen_plot.to_crs(proj)\n",
    "snpp_aspen_plot.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_snpp_plot_fired_events_west_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aceb58b-e1dd-40e3-9bb7-0fe102274ade",
   "metadata": {},
   "source": [
    "## Tidy the FRP data: remove null values, check on the obs./fire, and check on date matches\n",
    "\n",
    "Some observations may not be joined correctly (i.e., spatial overlap but wrong ignition year, etc). We may also have some fires with too few observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07377daf-08a7-4aef-9879-74ca09672a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the observation counts again\n",
    "viirs_counts = snpp_aspen_dis.groupby('fired_id').size().reset_index(name='counts')\n",
    "print(viirs_counts['counts'].min())\n",
    "print(viirs_counts['counts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c877-e71a-4894-9eee-0d33313752ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a map of the fire with the most observations\n",
    "\n",
    "# Sort the VIIRS counts\n",
    "viirs_counts = viirs_counts.sort_values('counts', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Take the first row (the maximum)\n",
    "max_obs = viirs_counts.iloc[0]['fired_id']\n",
    "print(max_obs)\n",
    "\n",
    "# Filter the fire perimeter and VIIRS obs.\n",
    "perim = fired_aspen[fired_aspen['fired_id'] == max_obs]\n",
    "obs = snpp_aspen_dis[snpp_aspen_dis['fired_id'] == max_obs]\n",
    "obs = obs.copy()\n",
    "obs['FRP_log'] = np.log1p(obs['FRP'])\n",
    "obs = obs[obs['DAYNIGHT'] == 'D']\n",
    "print(len(obs))\n",
    "\n",
    "# Create the map\n",
    "fig, ax = plt.subplots(figsize=(4, 5.5))\n",
    "# Plot VIIRS points\n",
    "obs.plot(column='FRP_log', ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Fire Radiative Power (FRP)\", 'orientation': \"horizontal\"},\n",
    "         cmap='magma', markersize=1, alpha=0.7)\n",
    "# Plot the fire perimeter\n",
    "perim.plot(ax=ax, color='none', edgecolor='black', linewidth=1, label='Fire Perimeter')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(maindir,'aspen-fire/Aim2/figures/FigX_MullenFire_FRP.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327045b-d443-41f9-ad51-ca789ebd4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frp_aspen_f['pct_aspen']))\n",
    "print(len(frp_aspen_f['FRP']))\n",
    "      \n",
    "# Scatterplot of FRP and aspen_pct (fire perimeter)\n",
    "plt.figure(figsize=(6, 4))  # Set the figure size\n",
    "plt.scatter(frp_aspen_f['pct_aspen'], frp_aspen_f['FRP'], alpha=0.5)  # Plot with some transparency\n",
    "\n",
    "# Add titles and labels\n",
    "plt.ylabel('Fire Radiative Power (FRP)')\n",
    "plt.xlabel('Aspen %')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812d666-ca41-46f6-9cfc-c9f7daf2d0df",
   "metadata": {},
   "source": [
    "## Join VIIRS observations to daily FIRED perimeters\n",
    "\n",
    "We want to summarize VIIRS observations on a daily basis and then associate them with the correct daily polygon from FIRED. The initial step is to group observations by day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec32dc-5938-4419-8477-d757f4c84ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the daily summary of FRP for each wildfire event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2658bb-7a19-4582-878b-f102e188f5fe",
   "metadata": {},
   "source": [
    "## Create the VIIRS observation buffer (375m2)\n",
    "\n",
    "The archive VIIRS data is distributed as shapefiles with centroids representing the pixel center of a VIIRS observation. In order to assess characteristics within the VIIRS observations, we want to create a 375m2 buffer around the centroid locations to approximate the VIIRS pixel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc88d3-d1dc-4f62-afe2-bef5c98f6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the buffered VIIRS obs.\n",
    "frp_aspen_plot = frp_aspen_f.copy()\n",
    "frp_aspen_plot['geometry'] = frp_aspen_plot.geometry.buffer(375, cap_style=3)  # square buffer 375m\n",
    "\n",
    "print(len(fired_aspen))\n",
    "\n",
    "# Let's plot one fire using the FRP column to color the \"plots\"\n",
    "\n",
    "# Filter the fire perimeter and VIIRS obs.\n",
    "perim = fired_aspen[fired_aspen['fired_id'] == \"42306\"]  # Williams Fork Fire \"45811.0\"\n",
    "obs = frp_aspen_plot[frp_aspen_plot['fired_id'] == \"42306\"]\n",
    "obs = obs.copy()\n",
    "obs['FRP_log'] = np.log1p(obs['FRP'])\n",
    "obs = obs[obs['DAYNIGHT'] == 'D']  # plot only daytime observations\n",
    "print(len(obs))\n",
    "\n",
    "# Create the map\n",
    "fig, ax = plt.subplots(figsize=(4, 5.5))\n",
    "# Plot VIIRS points\n",
    "obs.plot(column='FRP_log', ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Fire Radiative Power (FRP)\"},\n",
    "         cmap='magma', markersize=1, alpha=0.7)\n",
    "# Plot the fire perimeter\n",
    "perim.plot(ax=ax, color='none', edgecolor='black', linewidth=1, label='Fire Perimeter')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5ab75-9702-49e2-abb5-e92b3339605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = fired_aspen.copy()\n",
    "centroid['geometry'] = centroid.geometry.centroid\n",
    "\n",
    "# Make a spatial map of the centroids now\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "states.plot(ax=ax, edgecolor='black', linewidth=1, color='none')\n",
    "\n",
    "# Plot centroids\n",
    "centroid['size'] = centroid['pct_aspen'] * 10  # Adjust the scaling factor as necessary\n",
    "centroid.plot(\n",
    "    ax=ax, markersize=centroid['pct_aspen'], \n",
    "    column='pct_aspen', cmap='viridis', \n",
    "    legend=True, alpha=0.6, \n",
    "    legend_kwds={'label': \"Aspen Percent\"})\n",
    "\n",
    "# Optional: Plot the original fire perimeters for context\n",
    "fired_aspen.plot(ax=ax, color='none', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "\n",
    "del centroid\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(maindir,'aspen-fire/Aim2/figures/Fig1_aspen_fires.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bc2d2-ad96-45ad-ba17-cde5fdbf4db8",
   "metadata": {},
   "source": [
    "## Tidy and save out the necessary files\n",
    "\n",
    "Now that we have a tidy dataframe for both wildfires with >=5% pre-fire aspen cover and their associated nominal or high confidence VIIRS observations, we can save these files out for further processing. \n",
    "\n",
    "Some of the processing will occur in GEE, so for these files we want to save a simplified SHP with only the required attribute information (they will be joined back to the full data after processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96011e35-054b-4e03-b57d-d729af6828a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the observation counts again\n",
    "viirs_counts = frp_aspen_plot.groupby('fired_id').size().reset_index(name='counts')\n",
    "print(viirs_counts['counts'].min())\n",
    "print(viirs_counts['counts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71248f-2f9c-4e16-8d22-e0f81c299d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the daily files\n",
    "# Get the list of IDs\n",
    "ids = fired_aspen['fired_id'].unique()\n",
    "\n",
    "# Load the daily polygons, subset to aspen fires\n",
    "daily['id'] = daily['id'].astype(str)\n",
    "daily = daily[daily['id'].isin(ids)]\n",
    "print(len(daily['id'].unique()))\n",
    "\n",
    "# Save the daily wildfire perimeters\n",
    "daily = daily.to_crs(proj)  # ensure the correct projection before exporting\n",
    "daily.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_daily_west_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b85319-68dc-4899-b7ed-977424bd8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wildfire perimeters\n",
    "fired_aspen = fired_aspen.to_crs(proj)  # ensure the correct projection before exporting\n",
    "fired_aspen.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (points)\n",
    "frp_aspen_f = frp_aspen_f.to_crs(proj)\n",
    "frp_aspen_f.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_obs_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (plots)\n",
    "frp_aspen_plot = frp_aspen_plot.to_crs(proj)\n",
    "frp_aspen_plot.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_plots_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Tidy the files for GEE imports\n",
    "\n",
    "# FIRED perimeters (1km buffer)\n",
    "print(fired_aspen_1km.columns)\n",
    "fired_aspen_gee = fired_aspen_1km[['fired_id','ig_date','ig_year','last_date','mx_grw_dte','geometry']]\n",
    "fired_aspen_gee['ig_date'] = fired_aspen_gee['ig_date'].astype(str)\n",
    "fired_aspen_gee['last_date'] = fired_aspen_gee['ig_date'].astype(str)\n",
    "fired_aspen_gee.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/GEE/fired_events_west_aspen.shp'))\n",
    "\n",
    "# VIIRS \"plots\"\n",
    "print(frp_aspen_plot.columns)\n",
    "frp_aspen_gee = frp_aspen_plot[['fired_id','VID','ACQ_DATE','DAYNIGHT','geometry']]\n",
    "frp_aspen_gee['ACQ_DATE'] = frp_aspen_gee['ACQ_DATE'].astype(str)\n",
    "frp_aspen_gee.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/GEE/viirs_plots_fired_events_west_aspen.shp'))\n",
    "\n",
    "print(\"Success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
