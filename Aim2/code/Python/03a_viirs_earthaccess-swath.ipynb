{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec03ae-2946-4baf-a5f7-f722a615f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloading VIIRS Active Fire Detections (AFD) with 'earthaccess' python API\n",
    "\n",
    "Searches and accesses VIIRS AFD for fire perimeters.\n",
    "\n",
    "VIIRS/NPP Active Fires 6-Min L2 Swath 375m V002 (VNP14IMG)\n",
    "VIIRS/JPSS1 Active Fires 6-Min L2 Swath 375m V002 (VJ114IMG)\n",
    "\n",
    "Return: \n",
    "    - Downloaded/cloud access NetCDF granules for the above products\n",
    "    - GeoDataFrame representing active fire pixel locations and attributes (before geolocation)\n",
    "    - Geolocation grid representing pixel locations and overlap of adjacent orbits\n",
    "\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import earthaccess as ea\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from rasterio.transform import from_bounds\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "        \n",
    "# Directories\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "# Output directories\n",
    "dataraw = os.path.join(projdir,'data/spatial/raw/VIIRS/')\n",
    "datamod = os.path.join(projdir,'data/spatial/mod/VIIRS/')\n",
    "\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b7ebd1-d1ba-400d-8b65-d3b993c033ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class and functions ready !\n"
     ]
    }
   ],
   "source": [
    "class Access_VIIRS_AFD:\n",
    "    \"\"\" \n",
    "    Accesses VIIRS Active Fire Data (AFD) within a region for given date range\n",
    "    \"\"\"\n",
    "    def __init__(self, start_date, last_date, geom = gpd.GeoDataFrame(),\n",
    "                 id_col='Fire_ID', name_col='Fire_Name',\n",
    "                 geog_crs = 'EPSG:4326', proj_crs = 'EPSG:5070',\n",
    "                 short_names = ['VNP14IMG', 'VJ114IMG'],\n",
    "                 buffer = None, out_directory=None, \n",
    "                 processed_granules=None, \n",
    "                 region=None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - start_date: the intial date for the granule search\n",
    "            - last_date: the final date for the granule search\n",
    "            - geom: GeoDataFrame for search request (fire perimeter)\n",
    "            - geog_crs: Geographic projection (to retrieve coordinate pairs in lat/lon)\n",
    "            - proj_crs: Projected coordinate system\n",
    "            - short_names: the granules to be downloaded\n",
    "            - buffer: Optional buffer for input geometry\n",
    "            - out_directory: output directory to store results\n",
    "            - download: If 'True', downloads the netcdf, otherwise processes in cloud\n",
    "        Returns:\n",
    "            - Downloaded files (VIIRS Active Fire Data NetCDF and Geolocation information)\n",
    "            - GeoDataFrame with non-geolocated (raw) fire detections\n",
    "        \"\"\"\n",
    "        # Extract coordinate bounds\n",
    "        if region is None:\n",
    "            # use the fire perimeter\n",
    "            self.coords, self.extent = get_coords(geom, buffer)\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        elif region is not None and isinstance(region, gpd.GeoDataFrame):\n",
    "            # use the region boundary for FP and fire for search\n",
    "            _, self.extent = get_coords(region, buffer) # for extracting FP\n",
    "            self.coords, _ = get_coords(geom, buffer) # for data search\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        else:\n",
    "            print(\"Input region is not a GeoDataFrame !!!\")\n",
    "            \n",
    "        # Extract class attributes\n",
    "        self.fire_id = geom[id_col].iloc[0]\n",
    "        self.fire_name = geom[name_col].iloc[0]\n",
    "        self.date_range = (str(start_date), str(last_date))\n",
    "        self.geog_crs = geog_crs\n",
    "        self.proj_crs = proj_crs\n",
    "        self.short_names = short_names\n",
    "        self.out_dir = out_directory\n",
    "        self.granule_log = os.path.join(dataraw, 'logs/vnp_vji_processed_granules.txt')\n",
    "        self.processed_granules = processed_granules\n",
    "        self.lut = pd.read_csv(os.path.join(projdir, 'data/tabular/raw/pix_size_lut.csv'))\n",
    "      \n",
    "    def ea_search_request(self):\n",
    "        \"\"\" Generate an earthaccess search request with the given parameters \"\"\"\n",
    "        query = ea.search_data(\n",
    "            short_name=self.short_names, \n",
    "            polygon=self.coords,\n",
    "            temporal=self.date_range, \n",
    "            cloud_hosted=True,\n",
    "            count=-1\n",
    "        )\n",
    "        \n",
    "        # Grab a list of granules from the search query\n",
    "        granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier'] for g in query]\n",
    "        N = len(granules)\n",
    "    \n",
    "        # Filter the query to only work with the \"new\" granules\n",
    "        # Skip if no new granules are required\n",
    "        if self.processed_granules is not None:\n",
    "            processed = [g.replace('.nc', '') for g in self.processed_granules]\n",
    "            new_granules = [g for g in granules if g not in processed]\n",
    "            if len(new_granules) == 0:\n",
    "                print(f\"\\t! All granules already processed, skipping ... !\")\n",
    "                return None, None\n",
    "            elif len(new_granules) > 0 and len(new_granules) < N:\n",
    "                print(f\"\\n\\t! Some granules already processed [{N - len(new_granules)}] !\")\n",
    "                query = [item for item in query if item['umm']['DataGranule']['Identifiers'][0]['Identifier'] in new_granules]\n",
    "                granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier'] for g in query]\n",
    "            else:\n",
    "                print(f\"\\n\\t! Starting processing for [{len(granules)}] granules !\")\n",
    "                query = query\n",
    "                granules = granules\n",
    "\n",
    "        # open the fileset\n",
    "        fileset = ea.open(query)\n",
    "\n",
    "        # return query results and list of granules\n",
    "        return fileset, granules\n",
    "             \n",
    "\n",
    "    def create_fire_gdf(self, fileset):\n",
    "        \"\"\" Creates a geodataframe with active fire detections from a directory with NetCDF files \"\"\"\n",
    "\n",
    "        granule_dfs = [] # to store the geolocated AFDs\n",
    "        \n",
    "        nprint = 10 # print counter\n",
    "        for fp in tqdm(fileset, desc=\"Processing granules\"):\n",
    "            df = pd.DataFrame() # to store the active fire data\n",
    "            with xr.open_dataset(fp, phony_dims='access') as swath:\n",
    "                # make sure there are fire pixels\n",
    "                if swath.FirePix == 0:\n",
    "                    continue\n",
    "                \n",
    "                # get the granule ID and associated geolocation swath\n",
    "                granule_id = swath.LocalGranuleID\n",
    "                geo_id = swath.VNP03IMG\n",
    "                \n",
    "                # get the data variables\n",
    "                lonfp = swath.variables['FP_longitude'][:] # fire pixel longitude\n",
    "                latfp = swath.variables['FP_latitude'][:] # fire pixel latitude\n",
    "                frp = swath.variables['FP_power'][:] # fire radiative power\n",
    "                t4 = swath.variables['FP_T4'][:] # I04 brightness temp (kelvins)\n",
    "                t5 = swath.variables['FP_T5'][:] # I05 brightness temp (kelvins)\n",
    "                m13 = swath.variables['FP_Rad13'][:] # M13 radiance (kelvin)\n",
    "                sample = swath.variables['FP_sample'][:]\n",
    "                line = swath.variables['FP_line'][:]\n",
    "                # get the fire mask for fire pixels\n",
    "                fire_mask = swath['fire mask'][line, sample].values\n",
    "\n",
    "            # gather information from file name\n",
    "            timestamp = granule_id.split('.')[1:3]\n",
    "            year = timestamp[0][1:5]\n",
    "            day = timestamp[0][5:8]\n",
    "            acqtime = timestamp[1]\n",
    "            acqdate = dt.datetime.strptime(year+day, '%Y%j').strftime('%-m/%-d/%Y')\n",
    "            \n",
    "            df['longitude'] = lonfp\n",
    "            df['latitude'] = latfp\n",
    "            df['j'] = sample #sample number for pixel size lookup\n",
    "            df['fire_mask'] = fire_mask\n",
    "            df['confidence'] = pd.Categorical(df.fire_mask)\n",
    "            df.confidence = df.confidence.replace(\n",
    "                {0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "            df['frp'] = frp\n",
    "            df['t4'] = t4\n",
    "            df['t5'] = t5\n",
    "            df['m13'] = m13\n",
    "            df['acq_date'] = acqdate\n",
    "            df['acq_time'] = acqtime\n",
    "            df['daynight'] = swath.DayNightFlag\n",
    "            df['satellite'] = swath.PlatformShortName\n",
    "            df['short_name'] = swath.ShortName\n",
    "            df['granule_id'] = granule_id\n",
    "            df['geo_id'] = geo_id\n",
    "        \n",
    "            df = pd.merge(df, self.lut, left_on='j', right_on='sample', how='left')\n",
    "            df.drop(columns=['j'], inplace=True)\n",
    "            \n",
    "            granule_dfs.append(df) # append the granule dataframe\n",
    "\n",
    "            # write the granule id to the log file\n",
    "            with open(self.granule_log, 'a') as log_file:\n",
    "                log_file.write(f\"{granule_id}\\n\")\n",
    "\n",
    "            # save a csv file\n",
    "            out_dir = os.path.join(dataraw,\"granules/\")\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            df.to_csv(os.path.join(out_dir,f\"{granule_id[:-3]}.csv\"))\n",
    "\n",
    "        gc.collect() # clear out garbage\n",
    "        \n",
    "        # concatenate the out dfs\n",
    "        if len(granule_dfs) > 0:\n",
    "            fire_data = pd.concat(granule_dfs) # for the entire list of granules\n",
    "            return fire_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "print(\"Class and functions ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fda4e-77aa-449d-a1f6-a2539419007a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ed65d-83b6-4892-8253-e4b4158beab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1db138-d9da-4606-a1fe-7154b40c17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available attributes: \n",
      "Index(['Fire_ID', 'Fire_Name', 'NIFC_ACRES', 'FINAL_ACRES', 'pct_cover',\n",
      "       'INCIDENT_ID', 'INCIDENT_NAME', 'START_YEAR', 'CAUSE', 'DISCOVERY_DATE',\n",
      "       'DISCOVERY_DOY', 'WF_CESSATION_DATE', 'WF_CESSATION_DOY',\n",
      "       'STR_DESTROYED_TOTAL', 'STR_DAMAGED_TOTAL', 'STR_THREATENED_MAX',\n",
      "       'EVACUATION_REPORTED', 'PEAK_EVACUATIONS', 'WF_PEAK_AERIAL',\n",
      "       'WF_PEAK_PERSONNEL', 'na_l3name', 'first_obs_date', 'last_obs_date',\n",
      "       'obs_count', 'geometry'],\n",
      "      dtype='object')\n",
      "\n",
      "There are [70] fires.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fire_Name</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>WF_CESSATION_DATE</th>\n",
       "      <th>first_obs_date</th>\n",
       "      <th>last_obs_date</th>\n",
       "      <th>obs_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>577</td>\n",
       "      <td>2019-07-28</td>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>2019-07-30</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>416</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2018-07-06</td>\n",
       "      <td>2955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEBO</td>\n",
       "      <td>2020-10-13</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOADING PEN</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>2020-06-18</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>2020-06-18</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PLUMTAW</td>\n",
       "      <td>2022-05-17</td>\n",
       "      <td>2022-05-18</td>\n",
       "      <td>2022-05-17</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Fire_Name DISCOVERY_DATE WF_CESSATION_DATE first_obs_date last_obs_date  \\\n",
       "1          577     2019-07-28        2019-08-18     2019-07-30    2019-08-14   \n",
       "2          416     2018-06-01        2018-07-03     2018-06-01    2018-07-06   \n",
       "3         NEBO     2020-10-13        2020-10-15     2020-10-14    2020-10-15   \n",
       "4  LOADING PEN     2020-06-13        2020-06-18     2020-06-14    2020-06-18   \n",
       "5      PLUMTAW     2022-05-17        2022-05-18     2022-05-17    2022-05-19   \n",
       "\n",
       "   obs_count  \n",
       "1          2  \n",
       "2       2955  \n",
       "3          7  \n",
       "4          5  \n",
       "5         41  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fire dataset for the Southern Rockies\n",
    "fires = gpd.read_file(os.path.join(projdir,'data/spatial/mod/NIFC/nifc-ics_2018_to_2023-aspen-obs.gpkg'))\n",
    "fires = fires[fires['na_l3name'] == 'Southern Rockies']\n",
    "\n",
    "# tidy the fire id and name columns\n",
    "fires.rename(columns={'NIFC_ID': 'Fire_ID', 'NIFC_NAME': 'Fire_Name'}, inplace=True)\n",
    "fires['obs_count'] = fires['obs_count'].fillna(0).astype(int) # fill NaN as 0 obs.\n",
    "fires = fires[fires['obs_count'] != 0]\n",
    "# tidy the date columns\n",
    "fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE']).dt.date\n",
    "fires['WF_CESSATION_DATE'] = pd.to_datetime(fires['WF_CESSATION_DATE']).dt.date\n",
    "\n",
    "print(f\"Available attributes: \\n{fires.columns}\")\n",
    "print(f\"\\nThere are [{len(fires)}] fires.\")\n",
    "fires[['Fire_Name','DISCOVERY_DATE','WF_CESSATION_DATE','first_obs_date','last_obs_date','obs_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9351c3e9-2129-4c63-85f3-5293a6fb2890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       70.000000\n",
      "mean       868.000000\n",
      "std       2214.503196\n",
      "min          1.000000\n",
      "25%         10.250000\n",
      "50%         57.000000\n",
      "75%        562.250000\n",
      "max      12563.000000\n",
      "Name: obs_count, dtype: float64\n",
      "\n",
      "There are [54] fires with > 10 VIIRS obs.\n"
     ]
    }
   ],
   "source": [
    "print(fires['obs_count'].describe())\n",
    "fires = fires[fires['obs_count'] >= 10]\n",
    "print(f\"\\nThere are [{len(fires)}] fires with > 10 VIIRS obs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8af0edb-153d-45e8-9d84-35086aa3215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2018-06-29    6\n",
      "2018-06-30    6\n",
      "2018-08-12    5\n",
      "2018-08-15    5\n",
      "2018-07-30    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with individual dates for each fire\n",
    "date_counts = pd.DataFrame(\n",
    "    [(fire['Fire_ID'], single_date)\n",
    "     for _, fire in fires.iterrows()\n",
    "     for single_date in pd.date_range(fire['DISCOVERY_DATE'], fire['WF_CESSATION_DATE'])],\n",
    "    columns=['Fire_ID', 'Date']\n",
    ")['Date'].value_counts()\n",
    "print(date_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc3c956-82e0-4fe5-803c-138d4b5a2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start and end dates adjusted by 1 day ...\n"
     ]
    }
   ],
   "source": [
    "# Adjust the first and last date by one for the earthaccess search\n",
    "fires['first_obs_date'] = fires['first_obs_date'] - pd.Timedelta(days=1)\n",
    "fires['last_obs_date'] = fires['last_obs_date'] + pd.Timedelta(days=1)\n",
    "print(\"Start and end dates adjusted by 1 day ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b5cad-1f98-44bd-852e-abe5129f8a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fb18e-1db4-4def-aedd-8b833b51e26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c92ac8f-4196-4e9f-a815-b7e45965d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for fire perimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070823f2-cc03-4408-8b58-3467a1d4e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed [3241] granules.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for already processed granules\n",
    "granule_log = os.path.join(dataraw, 'logs/vnp_vji_processed_granules.txt')\n",
    "if os.path.exists(granule_log):\n",
    "    with open(granule_log, 'r') as log_file:\n",
    "        granules_p = set([line.strip() for line in log_file.readlines()])\n",
    "else:\n",
    "    granules_p = set()\n",
    "\n",
    "print(f\"Already processed [{len(granules_p)}] granules.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72c3128-d09b-4523-a0e9-bf90a63b5bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 416 fire:\n",
      "Granules found: 215\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BURRO fire:\n",
      "Granules found: 162\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BADGER_CREEK fire:\n",
      "Granules found: 148\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SARDINAS_CANYON fire:\n",
      "Granules found: 46\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SPRING_CREEK fire:\n",
      "Granules found: 91\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CHATEAU fire:\n",
      "Granules found: 36\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WESTON_PASS fire:\n",
      "Granules found: 47\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LAKE_CHRISTINE fire:\n",
      "Granules found: 181\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SILVER_CREEK fire:\n",
      "Granules found: 456\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SARCA fire:\n",
      "Granules found: 107\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WEST_GUARD fire:\n",
      "Granules found: 91\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CACHE_CREEK fire:\n",
      "Granules found: 129\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CABIN_LAKE fire:\n",
      "Granules found: 132\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BULL_DRAW fire:\n",
      "Granules found: 380\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for HORSE fire:\n",
      "Granules found: 180\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for RYAN fire:\n",
      "Granules found: 118\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DOE_CANYON fire:\n",
      "Granules found: 70\n",
      "\n",
      "\t! Some granules already processed [69] !\n",
      "Opening 1 granules, approx size: 0.0 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c003595aa64237ac02126f3db1dce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15036f6c40604cecaef49306c7ca72a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f191c967954fc58b7fdffdfe8a8f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8832f00c3346c9856a182c48989ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 6    DOE CANYON\n",
      "Name: Fire_Name, dtype: object: 0.11 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for BEAVER fire:\n",
      "Granules found: 71\n",
      "\n",
      "\t! Some granules already processed [19] !\n",
      "Opening 52 granules, approx size: 0.11 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f4349995254b5ba9d173c827680417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca2427d9c1d455387492f0e35053a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b422ba15be0d4ca9a652d860b886c659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f06d3c9e649d98ffb73e49f407d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 309    BEAVER\n",
      "Name: Fire_Name, dtype: object: 1.49 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for 441 fire:\n",
      "Granules found: 46\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MIDDLE_MAMM fire:\n",
      "Granules found: 350\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for AMOLE fire:\n",
      "Granules found: 98\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DECKER fire:\n",
      "Granules found: 255\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BRUSH_CREEK fire:\n",
      "Granules found: 18\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for COW_CREEK fire:\n",
      "Granules found: 53\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SAND_CREEK fire:\n",
      "Granules found: 108\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GRIZZLY_CREEK fire:\n",
      "Granules found: 113\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CAMERON_PEAK fire:\n",
      "Granules found: 469\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WILLIAMS_FORK fire:\n",
      "Granules found: 444\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MEDIO fire:\n",
      "Granules found: 128\n",
      "\n",
      "\t! Some granules already processed [102] !\n",
      "Opening 26 granules, approx size: 0.05 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f57911608d14693aea66b1270ac523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1ed2e15f3b4ad2bb2794b2e98539e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de4995015fe4f50a57f239078e85e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202b4c85dda24f07a488853db4053d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 94    MEDIO\n",
      "Name: Fire_Name, dtype: object: 0.78 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for MIDDLE_FORK fire:\n",
      "Granules found: 296\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MULLEN fire:\n",
      "Granules found: 203\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for EAST_TROUBLESOME fire:\n",
      "Granules found: 75\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LUNA fire:\n",
      "Granules found: 54\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LEFTHAND fire:\n",
      "Granules found: 18\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for ICE fire:\n",
      "Granules found: 29\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PACK_CREEK fire:\n",
      "Granules found: 88\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for RINCON fire:\n",
      "Granules found: 30\n",
      "\n",
      "\t! Some granules already processed [24] !\n",
      "Opening 6 granules, approx size: 0.01 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910bb8f71bb0494c963a236881283c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1126b2cb08584cc392a300a08f337d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7d8c8aff8449df9341ff66e60f388f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfa9d6e328d43049965b8433d51ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 96    RINCON\n",
      "Name: Fire_Name, dtype: object: 0.28 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for SYLVAN fire:\n",
      "Granules found: 23\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MUDDY_SLIDE fire:\n",
      "Granules found: 25\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MORGAN_CREEK fire:\n",
      "Granules found: 129\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BLACK_MOUNTAIN fire:\n",
      "Granules found: 24\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CALF_CANYON fire:\n",
      "Granules found: 447\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CERRO_PELADO fire:\n",
      "Granules found: 172\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PLUMTAW fire:\n",
      "Granules found: 23\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SUGARLOAF fire:\n",
      "Granules found: 39\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for 403 fire:\n",
      "Granules found: 24\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for COMANCHE fire:\n",
      "Granules found: 38\n",
      "\n",
      "\t! Starting processing for [38] granules !\n",
      "Opening 38 granules, approx size: 0.08 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fdb831a4f2486cbc52f37863a22439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d30ebe86aff4f48a8794555c94298c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263bc98618124dd5ac092b4228dfa145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454b545aee614276b7a37e0b3ea8a2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 281    COMANCHE\n",
      "Name: Fire_Name, dtype: object: 1.11 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for CHRIS_MOUNTAIN fire:\n",
      "Granules found: 28\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DRY_LAKE fire:\n",
      "Granules found: 22\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for QUARTZ_RIDGE fire:\n",
      "Granules found: 215\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BEAR_CREEK fire:\n",
      "Granules found: 213\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BLACK_FEATHER fire:\n",
      "Granules found: 21\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for TRAIL_SPRINGS fire:\n",
      "Granules found: 160\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MILL_CREEK_2 fire:\n",
      "Granules found: 112\n",
      "\t! All granules already processed, skipping ... !\n",
      "\n",
      "Total elapsed time: 5.58 minutes.\n",
      "\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()   \n",
    "\n",
    "# Get a list of fire IDs sorted by ignition date\n",
    "fires = fires.sort_values(by=['START_YEAR','first_obs_date'])\n",
    "fire_ids = fires['Fire_ID'].unique()\n",
    "\n",
    "afd_dfs = [] # to store the output geodataframes\n",
    "\n",
    "# Loop fire ids\n",
    "for fire_id in fire_ids:\n",
    "    t00 = time.time()\n",
    "\n",
    "    fire = fires[fires['Fire_ID'] == fire_id]\n",
    "    fire_name = fire['Fire_Name'].iloc[0]\n",
    "    fire_name = fire_name.replace(\" \", \"_\")\n",
    "    print(f\"Processing for {fire_name} fire:\")\n",
    "    \n",
    "    da_access = Access_VIIRS_AFD(\n",
    "        start_date=fire['first_obs_date'].iloc[0],\n",
    "        last_date=fire['last_obs_date'].iloc[0],\n",
    "        geom=fire,\n",
    "        buffer=1000,\n",
    "        short_names=['VNP14IMG','VJ114IMG'],\n",
    "        out_directory=dataraw,\n",
    "        processed_granules=granules_p\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        fileset, granules = da_access.ea_search_request()\n",
    "\n",
    "        if granules is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n\\tExtracting active fires ...\\n\")\n",
    "        afd_fire = da_access.create_fire_gdf(fileset)\n",
    "\n",
    "        # save the progress so far\n",
    "        if afd_fire is not None:\n",
    "            afd_dfs.append(afd_fire)\n",
    "            granules_p.update(granules) # running list\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping fire {fire['Fire_Name']}\\n{e}\\n\")\n",
    "        traceback.print_exc()  # This will print the full traceback\n",
    "        continue # continue to the next fire id\n",
    "\n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"\\nTotal elapsed time for {fire['Fire_Name']}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"\\nTotal elapsed time: {t2:.2f} minutes.\\n\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16049c96-23fe-411a-ae8e-ee111842b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3364\n"
     ]
    }
   ],
   "source": [
    "granules_ = glob.glob(os.path.join(dataraw,'granules/*.csv'))\n",
    "print(len(granules_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ecdf797-cd61-4cd6-91d1-a355a4eb4956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>fire_mask</th>\n",
       "      <th>confidence</th>\n",
       "      <th>frp</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>m13</th>\n",
       "      <th>acq_date</th>\n",
       "      <th>...</th>\n",
       "      <th>daynight</th>\n",
       "      <th>satellite</th>\n",
       "      <th>short_name</th>\n",
       "      <th>granule_id</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>sample</th>\n",
       "      <th>along_scan</th>\n",
       "      <th>along_track</th>\n",
       "      <th>scan_angle</th>\n",
       "      <th>pix_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-95.160160</td>\n",
       "      <td>34.074640</td>\n",
       "      <td>9</td>\n",
       "      <td>h</td>\n",
       "      <td>14.550305</td>\n",
       "      <td>367.00000</td>\n",
       "      <td>295.69968</td>\n",
       "      <td>2.373925</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>924</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>47.8431</td>\n",
       "      <td>0.224667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-95.164330</td>\n",
       "      <td>34.074383</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>14.550305</td>\n",
       "      <td>350.57697</td>\n",
       "      <td>296.18445</td>\n",
       "      <td>2.373925</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>925</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>47.8342</td>\n",
       "      <td>0.224486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-95.160630</td>\n",
       "      <td>34.080082</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>6.089355</td>\n",
       "      <td>343.91302</td>\n",
       "      <td>296.12167</td>\n",
       "      <td>1.406672</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>924</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>47.8431</td>\n",
       "      <td>0.224667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-95.164825</td>\n",
       "      <td>34.079823</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>6.089355</td>\n",
       "      <td>345.46840</td>\n",
       "      <td>296.05000</td>\n",
       "      <td>1.406672</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>925</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>47.8342</td>\n",
       "      <td>0.224486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-95.906006</td>\n",
       "      <td>34.813230</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>4.297072</td>\n",
       "      <td>340.28574</td>\n",
       "      <td>296.18735</td>\n",
       "      <td>1.041121</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>46.4736</td>\n",
       "      <td>0.199456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  longitude   latitude  fire_mask confidence        frp  \\\n",
       "0           0 -95.160160  34.074640          9          h  14.550305   \n",
       "1           1 -95.164330  34.074383          8          n  14.550305   \n",
       "2           2 -95.160630  34.080082          8          n   6.089355   \n",
       "3           3 -95.164825  34.079823          8          n   6.089355   \n",
       "4           4 -95.906006  34.813230          8          n   4.297072   \n",
       "\n",
       "          t4         t5       m13   acq_date  ...  daynight satellite  \\\n",
       "0  367.00000  295.69968  2.373925  6/27/2019  ...       Day    JPSS-1   \n",
       "1  350.57697  296.18445  2.373925  6/27/2019  ...       Day    JPSS-1   \n",
       "2  343.91302  296.12167  1.406672  6/27/2019  ...       Day    JPSS-1   \n",
       "3  345.46840  296.05000  1.406672  6/27/2019  ...       Day    JPSS-1   \n",
       "4  340.28574  296.18735  1.041121  6/27/2019  ...       Day    JPSS-1   \n",
       "\n",
       "  short_name                                   granule_id  \\\n",
       "0   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "1   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "2   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "3   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "4   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "\n",
       "                                        geo_id sample  along_scan  \\\n",
       "0  VJ103IMG.A2019178.2006.021.2021049184623.nc    924    0.381910   \n",
       "1  VJ103IMG.A2019178.2006.021.2021049184623.nc    925    0.381694   \n",
       "2  VJ103IMG.A2019178.2006.021.2021049184623.nc    924    0.381910   \n",
       "3  VJ103IMG.A2019178.2006.021.2021049184623.nc    925    0.381694   \n",
       "4  VJ103IMG.A2019178.2006.021.2021049184623.nc   1078    0.351243   \n",
       "\n",
       "   along_track  scan_angle  pix_area  \n",
       "0     0.588271     47.8431  0.224667  \n",
       "1     0.588131     47.8342  0.224486  \n",
       "2     0.588271     47.8431  0.224667  \n",
       "3     0.588131     47.8342  0.224486  \n",
       "4     0.567857     46.4736  0.199456  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afds = pd.concat((pd.read_csv(f) for f in granules_), ignore_index=True)\n",
    "afds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae39719-8b80-4cbc-bda2-2a19434106de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1849132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(afds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b83e6b4-89c7-43d7-b56a-336ce457f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JPSS-1', 'SUOMI-NPP'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afds['satellite'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84fc788b-23eb-4c01-81f0-6efbc4091b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Users/max/Library/CloudStorage/OneDrive-Personal/mcook/aspen-fire/Aim2/data/spatial/raw/VIIRS/viirs_snpp_jpss1_afd_.csv\n"
     ]
    }
   ],
   "source": [
    "# save the file.\n",
    "out_fp = os.path.join(dataraw, f'viirs_snpp_jpss1_afd_.csv')\n",
    "afds.to_csv(out_fp)\n",
    "print(f\"Saved to: {out_fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
