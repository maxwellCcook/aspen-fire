{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d797da2-4f14-4307-bafa-ef51add6105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify wildfire events with >= 1% pre-fire aspen forest cover across western U.S. ecoregions\n",
    "Landcover data: LANDFIRE Existing Vegetation Type (EVT) ca. 2016\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import os, time, glob, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "proj = 'EPSG:5070'\n",
    "\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "print(\"Ready to go !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a364ddd-b270-4978-b306-7c4d79cb2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded !\n"
     ]
    }
   ],
   "source": [
    "def create_bounds(gdf, buffer=None):\n",
    "    \"\"\" Calculate a bounding rectangle for a given geometry and buffer \"\"\"\n",
    "    bounds = gdf.geometry.apply(lambda geom: box(*geom.bounds))\n",
    "    if buffer is not None:\n",
    "        bounds = bounds.buffer(buffer)\n",
    "    # Assign the geometry to the geodataframe\n",
    "    gdf_ = gdf.copy()\n",
    "    gdf_.geometry = bounds\n",
    "    return gdf\n",
    "\n",
    "class BandStatistics:\n",
    "    def __init__(self, geo_fp, img_fp, uid):\n",
    "        \"\"\"\n",
    "        Initializes the BandStatistics object\n",
    "        \"\"\"\n",
    "        self.geometries = gpd.read_file(geo_fp)\n",
    "        self.image_da = rxr.open_rasterio(img_fp)\n",
    "        self.nodataval = self.image_da.rio.nodata\n",
    "        self.band_desc = 'evt' # single band output\n",
    "        self.id_col = str(uid)\n",
    "\n",
    "        # Check the CRS information matches\n",
    "        if self.image_da.rio.crs != self.geometries.crs:\n",
    "            self.geometries = self.geometries.to_crs(self.image_da.rio.crs)\n",
    "\n",
    "    def compute_band_stats(self, geom, band):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        affine = self.image_da.rio.transform()\n",
    "        stats = zonal_stats(\n",
    "            vectors=geom[[self.id_col,'geometry']], \n",
    "            raster=band, \n",
    "            categorical=True, \n",
    "            affine=affine,\n",
    "            all_touched=True,\n",
    "            nodata=self.nodataval,\n",
    "            geojson_out=True\n",
    "        )\n",
    "        # Convert the list of dicts to a GeoDataFrame\n",
    "        stats_df = pd.DataFrame(stats)\n",
    "        stats_df[self.id_col] = stats_df['properties'].apply(lambda x: x.get(self.id_col))\n",
    "        stats_df['properties'] = stats_df['properties'].apply(lambda x: {key: val for key, val in x.items() if key != self.id_col})\n",
    "        stats_df['props_list'] = stats_df['properties'].apply(lambda x: list(x.items()))\n",
    "\n",
    "        # Explode the list of categorical values into separate rows\n",
    "        props = stats_df.explode('properties_list').reset_index(drop=True)\n",
    "        # Split the list items into two new columns (land cover class and pixel count)\n",
    "        props[['evt', 'count']] = pd.DataFrame(props['properties_list'].tolist(), index=props.index)\n",
    "        # Select the relevant columns\n",
    "        props = props[[self.id_col, 'evt', 'count']].reset_index(drop=True)\n",
    "\n",
    "        # Calculate the total pixel count per geometry\n",
    "        total_pixels = props.groupby(props[self.id_col])['count'].transform('sum')\n",
    "        # Calculate percentage cover for each class\n",
    "        props['total_pixels'] = total_pixels\n",
    "        props['pct_cover'] = (props['count'] / props['total_pixels']) * 100\n",
    "\n",
    "        del stats_gdf, affine\n",
    "        gc.collect() # clean up\n",
    "        return props\n",
    "\n",
    "    def process_chunk(self, chunk):\n",
    "        \"\"\"\n",
    "        Processes a chunk of geometries for land cover statistics\n",
    "        Args:\n",
    "            - chunk: a subset of geometries to process\n",
    "        \"\"\"\n",
    "        band_da = self.image_da.values\n",
    "        stats = self.compute_band_stats(chunk, band_da)\n",
    "        return stats\n",
    "    \n",
    "    def parallel_compute_stats(self):\n",
    "        \"\"\"\n",
    "        Parallelizes the categorical statistics computation for all geometries for the single band.\n",
    "        Automatically sets the number of workers to the number of available CPU cores minus one.\n",
    "        \"\"\"\n",
    "        num_workers = max(1, os.cpu_count() -1)\n",
    "        print(f\"Using {num_workers} workers.\")\n",
    "\n",
    "        # Split geometries into chunks for parallel processing\n",
    "        chunks = np.array_split(self.geometries, num_workers)\n",
    "\n",
    "        with mp.Pool(processes=num_workers) as pool:\n",
    "            results = pool.starmap(self.process_chunk, [(chunk,) for chunk in chunks])\n",
    "        \n",
    "        results_df = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "\n",
    "print(\"Functions loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b02c4c-ef7e-4b47-8dd1-1aad328f5fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5467af0e-7f57-45dc-b750-89b962c788c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the land cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b60e93-b5b4-4c2a-9ecb-5480e03889ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (97283, 154207); \n",
      "GeoTransform: -2362425.0 30.0 0.0 3177435.0 0.0 -30.0; \n",
      "WKT: EPSG:5070; \n",
      "NoData Value: nan; \n",
      "Data Type: float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LANDFIRE EVT (ca. 2016)\n",
    "evt_fp = os.path.join(maindir,'data/landcover/LANDFIRE/LF2016_EVT_200_CONUS/Tif/LC16_EVT_200.tif')\n",
    "evt = rxr.open_rasterio(evt_fp, masked=True, cache=False).squeeze()\n",
    "shp, gt, wkt, nd = evt.shape, evt.spatial_ref.GeoTransform, evt.rio.crs, evt.rio.nodata\n",
    "print(\n",
    "    f\"Shape: {shp}; \\n\"\n",
    "    f\"GeoTransform: {gt}; \\n\"\n",
    "    f\"WKT: {wkt}; \\n\"\n",
    "    f\"NoData Value: {nd}; \\n\"\n",
    "    f\"Data Type: {evt[0].dtype}\")\n",
    "del evt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132327c-dfba-49fb-b5d2-28d9372cdd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8edeb6-699f-4e55-8ea3-e334c172787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FIRED perimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb5b1a-c60d-44ac-96be-6b39f78160df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9bcab2-f7ef-4284-ac4f-acef2d28a0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['did', 'id', 'date', 'ig_date', 'ig_day', 'ig_month', 'ig_year',\n",
       "       'last_date', 'event_day', 'event_dur', 'pixels', 'tot_pix', 'dy_ar_km2',\n",
       "       'tot_ar_km2', 'fsr_px_dy', 'fsr_km2_dy', 'mx_grw_px', 'mn_grw_px',\n",
       "       'mu_grw_px', 'mx_grw_km2', 'mn_grw_km2', 'mu_grw_km2', 'mx_grw_dte',\n",
       "       'x', 'y', 'ig_utm_x', 'ig_utm_y', 'lc_code', 'lc_mode', 'lc_name',\n",
       "       'lc_desc', 'lc_type', 'eco_mode', 'eco_name', 'eco_type', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the FIRED perimeters (2018-2024)\n",
    "daily_fp = os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired-daily_west_2018_to_2024.gpkg')\n",
    "daily = gpd.read_file(daily_fp)\n",
    "daily = daily.to_crs(proj) # ensure albers projection\n",
    "daily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafce3a-a7da-47fd-8422-9f65557eef9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881aeeb-3b03-41a0-b691-70d5ace3d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily land cover (EVT) proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367193f-3d3b-49ea-bf25-3fed27f7bc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad702c12-1d78-40f1-bebc-93f45cf16d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/aspen-fire/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'GeoDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'GeoDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m band_stats_obj \u001b[38;5;241m=\u001b[39m BandStatistics(daily_fp, evt_fp, uid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run parallel band statistics computation\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sampled \u001b[38;5;241m=\u001b[39m \u001b[43mband_stats_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_compute_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m t1 \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mBandStatistics.parallel_compute_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m chunks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometries, num_workers)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 84\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/aspen-fire/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Create the band stats class\n",
    "band_stats_obj = BandStatistics(daily_fp, evt_fp, uid='did')\n",
    "# Run parallel band statistics computation\n",
    "sampled = band_stats_obj.parallel_compute_stats()\n",
    "\n",
    "t1 = (time.time() - t0) / 60\n",
    "print(f\"Total elapsed time: {t1:.2f} minutes.\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad74002-273a-4471-b784-5b55427ab9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e8d3aa6-c59b-4cb0-a4c5-a8146b306f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8401 unique fires.\n",
      "\t[27942] individual daily perimeters.\n"
     ]
    }
   ],
   "source": [
    "# Extract fires between 2018-2022\n",
    "daily_ = daily[(daily['ig_year'] >= 2018) & (daily['ig_year'] <= 2022)]\n",
    "print(f\"There are {len(daily_['id'].unique())} unique fires.\\n\\t[{len(daily_)}] individual daily perimeters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc86040-336c-4866-b286-269c5162b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a more broad scale approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3d4f8e-98d5-4367-a64a-54d8042225c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new geometry (bounds)\n",
    "bounds = create_bounds(fired, buffer=3000)\n",
    "bounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e5951-27bc-4b10-a21f-341dee042208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621bda4-ac2a-4fb7-a54e-93017703e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LANDFIRE Existing Vegetation Type (EVT) ca. 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651ed88-7c99-4a01-a1ec-fd402cbe4b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96180f4-18df-4a36-874a-2a599c466d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82591d07-86e6-47cf-b9bc-7128232a95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent cover from the LANDFIRE EVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5212b9-fbca-42fb-882c-57852a798777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdcf556b-a298-4834-b203-86cedad952f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10364 fire events.\n",
      "Index(['id', 'type', 'properties', 'geometry', 'bbox'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     17\u001b[0m t1 \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;66;03m# minutes\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal elapsed time for parameter combination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mi\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m zs, evt, shp, gt, wkt, nd\n\u001b[1;32m     21\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Categorical zonal statistics\n",
    "zs = zonal_stats(\n",
    "    vectors=bounds[['id','geometry']], \n",
    "    raster=evt_fp, \n",
    "    categorical=True, \n",
    "    geojson_out=True\n",
    ")\n",
    "\n",
    "# Extract as a geodataframe\n",
    "stats = gpd.GeoDataFrame(zs).fillna(0)\n",
    "\n",
    "print(f\"There are {len(stats['id'].unique())} fire events.\")\n",
    "print(stats.columns)\n",
    "\n",
    "t1 = (time.time() - t0) / 60 # minutes\n",
    "print(f\"Elapsed time: {t1:.2f} minutes.\")\n",
    "\n",
    "del zs, evt, shp, gt, wkt, nd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd67f2-4d3b-4d53-a400-e147d16ce591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the properties json\n",
    "\n",
    "# First get the fired_id\n",
    "stats['fired_id'] = stats['properties'].apply(lambda x: x.get('fired_id'))\n",
    "# Retrieve the other properties (EVT codes)\n",
    "stats['properties'] = stats['properties'].apply(lambda x: {key: val for key, val in x.items() if key != 'fired_id'})\n",
    "stats['properties_list'] = stats['properties'].apply(lambda x: list(x.items()))\n",
    "\n",
    "# Explode the json object\n",
    "props = stats.explode('properties_list').reset_index(drop=True)\n",
    "# retrieve the list items as new columns\n",
    "props[['EVT', 'pixel_count']] = pd.DataFrame(props['properties_list'].tolist(), index=props.index)\n",
    "props = props[['fired_id','EVT','pixel_count']].reset_index(drop=True)\n",
    "\n",
    "# Calculate the percent of each class\n",
    "total_pixels = props.groupby(props['fired_id'])['pixel_count'].transform('sum')\n",
    "props['total_pixels'] = total_pixels\n",
    "props['pct_cover'] = (props['pixel_count'] / props['total_pixels']) * 100\n",
    "\n",
    "print(props.head())\n",
    "print(props.columns)\n",
    "print(len(props['fired_id'].unique()))\n",
    "\n",
    "del stats, total_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983c164-e6cd-4a07-8f47-01cc801e5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lookup table for the EVT codes\n",
    "lookup = os.path.join(maindir,'data/landcover/LANDFIRE/LF2016_EVT_200_CONUS/CSV_Data/LF16_EVT_200.csv')\n",
    "lookup = pd.read_csv(lookup)\n",
    "print(lookup.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457a4ca-8f8b-45ec-ad43-a4b13fec6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the codes we want to join, join back to the dataframe\n",
    "lookup = lookup[['VALUE','EVT_NAME','EVT_PHYS','EVT_GP_N','EVT_CLASS']]\n",
    "# Merge back to the data\n",
    "props_df = props.merge(lookup, left_on='EVT', right_on='VALUE', how='left')\n",
    "\n",
    "print(props_df.head())\n",
    "print(len(props_df['fired_id'].unique()))\n",
    "\n",
    "del lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b35a3-30e9-442f-9349-1d46b1867d29",
   "metadata": {},
   "source": [
    "### Identify fires with at least 5% aspen forest cover\n",
    "\n",
    "Now that we have a data frame of landcover types for each wildfire, we can isolate the quaking aspen cover types and identify wildfires which had at least 5% pre-fire aspen forest cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc99ea-1324-48c7-b6ae-86d08ab0efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classes with 'aspen' in the EVT_NAME\n",
    "aspen_evt = props_df[props_df['EVT_NAME'].str.contains('aspen', case=False, na=False)]['EVT_NAME'].unique()\n",
    "print(aspen_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72cb6e-5d33-4718-b74e-a464714e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter to retain wildfire events with at least 5% aspen forest cover\n",
    "props_filtered = props_df[props_df['EVT_NAME'].isin(aspen_evt)]\n",
    "aspen_sum = props_filtered.groupby('fired_id')['pct_cover'].sum().reset_index() # get the sum of aspen classes\n",
    "aspen_sum['pct_aspen'] = aspen_sum['pct_cover']  # rename the column to retain aspen percent\n",
    "\n",
    "# Filter out fires with less than 5% aspen cover\n",
    "aspen_fires = aspen_sum[aspen_sum['pct_aspen'] >= 5] # retain fires with >= 5% \n",
    "aspen_fires = aspen_fires[['fired_id','pct_aspen']] # subset columns\n",
    "print(aspen_fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724fc10-3542-450b-af07-484ecc16f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back to the FIRED data\n",
    "# Check for matching fired_id set\n",
    "common_ids = set(fired['fired_id']).intersection(set(aspen_fires['fired_id'])) # Find intersection\n",
    "print(f\"Number of common IDs: {len(common_ids)}\")\n",
    "\n",
    "# Join the attribute data to FIRED polygons\n",
    "fired['fired_id'] = fired['fired_id'].astype(str)\n",
    "aspen_fires['fired_id'] = aspen_fires['fired_id'].astype(str)\n",
    "\n",
    "# Join aspen percent to the FIRED data\n",
    "fired_aspen = pd.merge(fired, aspen_fires, on='fired_id', how='inner')\n",
    "print(fired_aspen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec8e89-cd48-4c0a-8749-0eff2918446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fires that are only one MODIS pixel\n",
    "fired_aspen = fired_aspen[fired_aspen['tot_pix'] > 1]\n",
    "print(fired_aspen['tot_pix'].describe())\n",
    "# Filter out any \"cropland\" fires\n",
    "fired_aspen = fired_aspen[fired_aspen['lc_name'] != 'Croplands']\n",
    "print(fired_aspen['lc_name'].unique())\n",
    "\n",
    "print(f'There are {len(fired_aspen)} fire events meeting our criteria so far.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30419d9c-84d1-4da9-911b-ab6cc120ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this file out\n",
    "fired_aspen = fired_aspen.to_crs(proj)  # ensure the correct projection before exporting\n",
    "fired_aspen.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_events_west_aspen_all_gt5pct.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99305123-b1c3-4a01-8430-f5e6c0fce738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial map of aspen wildfires (centroid)\n",
    "\n",
    "# Load the state boundaries\n",
    "states = gpd.read_file(os.path.join(maindir,'data/boundaries/political/TIGER/tl19_us_states_west_nad83.gpkg'))\n",
    "\n",
    "# Generate centroids\n",
    "centroid = fired_aspen.copy()\n",
    "centroid['geometry'] = centroid.geometry.centroid\n",
    "\n",
    "# Make a spatial map of the centroids now\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "states.plot(ax=ax, edgecolor='black', linewidth=1, color='none')\n",
    "\n",
    "# Plot centroids\n",
    "centroid['size'] = centroid['pct_aspen'] * 10  # Adjust the scaling factor as necessary\n",
    "centroid.plot(\n",
    "    ax=ax, markersize=centroid['pct_aspen'], \n",
    "    column='pct_aspen', cmap='viridis', \n",
    "    legend=True, alpha=0.6, \n",
    "    legend_kwds={'label': \"Aspen Percent\"})\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "\n",
    "del centroid\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(projdir, 'figures/Fig1_all_aspen_fires.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95a47c-1c54-488c-8e56-9524bcf30666",
   "metadata": {},
   "source": [
    "#### Extract the daily fire perimeters for the aspen wildfire subset\n",
    "\n",
    "We need to also extract the daily fire perimeters from FIRED so we can perform further analysis on the temporal patterns of fire growth as it relates to aspen cover, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f8523-d1f4-466e-9b20-068f6cfc8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of IDs\n",
    "ids = fired_aspen['fired_id'].unique()\n",
    "\n",
    "# Load the daily polygons, subset to aspen fires\n",
    "daily_path = os.path.join(maindir,'FIRED/data/spatial/raw/events/events_040324/shapefiles/fired_conus_ak_2000_to_2024_daily.gpkg')\n",
    "daily = gpd.read_file(daily_path)\n",
    "daily['id'] = daily['id'].astype(str)\n",
    "daily = daily[daily['id'].isin(ids)]\n",
    "print(len(daily['id'].unique()))\n",
    "\n",
    "# Save out\n",
    "daily = daily.to_crs(proj)  # ensure the correct projection before exporting\n",
    "daily.to_file(os.path.join(projdir,'data/spatial/mod/FIRED/fired_daily_west_aspen_all_gt5pct.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed7417-3d84-41b8-96b7-2378b645f9e8",
   "metadata": {},
   "source": [
    "## Wrangle the MODIS 1km and VIIRS 375m Active Fire Detections (AFD)\n",
    "\n",
    "We downloaded the archive AFD for MODIS Collection 6.1 (1km), the Suomi National Polar-Orbiting Partnership (VIIRS S-NPP 375m) and NOAA-20 (VIIRS NOAA-20 375m) data products from the NASA FIRMS (https://firms.modaps.eosdis.nasa.gov/download/) between 2018-2023 in the western US. \n",
    "\n",
    "The VIIRS observations are split into archive (2018-2022) and \"NRT\" (2022-2023). These files need to be merged prior to performing the tidying.\n",
    "\n",
    "To start with, we will create a tidy database of VIIRS observations for both the S-NPP and NOAA-20 vintages. Then, we will look at creating a combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6fade-c645-48c0-a9ff-b3d132d0149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Set up the file paths\n",
    "\n",
    "modis = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_M-C61_476781/')\n",
    "snpp = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_SV-C2_476784/')\n",
    "noaa = os.path.join(projdir,'data/spatial/raw/NASA-FIRMS/DL_FIRE_J1V-C2_476782/')\n",
    "\n",
    "# Store these in a dictionary\n",
    "dict = {\n",
    "    \"MOD61\": modis,\n",
    "    \"SNPP\": snpp,\n",
    "    \"NOAA-20\": noaa\n",
    "}\n",
    "\n",
    "# Buffer fire perimeters for the extraction\n",
    "fire_buffer = fired_aspen.copy().to_crs(proj)\n",
    "fire_buffer['geometry'] = fire_buffer.geometry.buffer(1000)  # 1km buffer\n",
    "fire_buffer = fire_buffer[['fired_id','ig_date','last_date','geometry']] \n",
    "\n",
    "# Process each of the archive data products\n",
    "gdfs = {} # dictionary to store the geo data frames\n",
    "for key, path in dict.items():\n",
    "    print(f'Processing: {key}')\n",
    "    # Read in the archive vector data\n",
    "    vect = glob.glob(path+\"*.shp\")\n",
    "    print([os.path.basename(v) for v in vect])\n",
    "    if len(vect) > 1:\n",
    "        print(\"~Merging archive and NRT data.\")\n",
    "        archive = gpd.read_file([v for v in vect if \"archive\" in v][0]).to_crs(proj)\n",
    "        nrt = gpd.read_file([v for v in vect if \"nrt\" in v][0]).to_crs(proj)\n",
    "        # Concatenate the archive and NRT\n",
    "        afd = pd.concat([archive, nrt], ignore_index=True)\n",
    "        del archive, nrt\n",
    "    else:\n",
    "        afd = gpd.read_file(vect[0]).to_crs(proj)\n",
    "\n",
    "    # Add some attribute information\n",
    "    afd['VID'] = afd.index # unique ID column\n",
    "\n",
    "    # Remove low-confidence observations\n",
    "    try:\n",
    "        afd = afd[afd['CONFIDENCE'] != 'l']\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "\n",
    "    del vect\n",
    "\n",
    "    #################################################\n",
    "    # Extract AFDs within wildfire data (aspen fires)\n",
    "    \n",
    "    # Extract AFDs\n",
    "    afd_aspen = gpd.sjoin(afd, fire_buffer, how='inner', predicate='within')\n",
    "    print(afd_aspen.columns)\n",
    "    \n",
    "    del afd\n",
    "    \n",
    "    ####################################################\n",
    "    # Perform temporal filtering to remove false-positive matches\n",
    "\n",
    "    # First, create date columns\n",
    "    afd_aspen['ACQ_DATE'] = pd.to_datetime(afd_aspen['ACQ_DATE'])\n",
    "    afd_aspen['ACQ_MONTH'] = afd_aspen['ACQ_DATE'].dt.month.astype(int)\n",
    "    afd_aspen['ACQ_YEAR'] = afd_aspen['ACQ_DATE'].dt.year.astype(int)\n",
    "    afd_aspen['ig_date'] = pd.to_datetime(afd_aspen['ig_date'])\n",
    "    afd_aspen['last_date'] = pd.to_datetime(afd_aspen['last_date'])\n",
    "\n",
    "    # Filter based on ignition month and year\n",
    "    afd_aspen_f = afd_aspen[\n",
    "        (afd_aspen['ACQ_YEAR'] >= afd_aspen['ig_date'].dt.year.astype(int)) & \n",
    "        (afd_aspen['ACQ_MONTH'] >= afd_aspen['ig_date'].dt.month.astype(int)) &\n",
    "        (afd_aspen['ACQ_YEAR'] <= afd_aspen['last_date'].dt.year.astype(int)) &\n",
    "        (afd_aspen['ACQ_MONTH'] <= afd_aspen['last_date'].dt.month.astype(int))\n",
    "    ]\n",
    "    \n",
    "    # Keep unique rows\n",
    "    afd_aspen_f = afd_aspen_f.drop_duplicates(subset='VID', keep='first')    \n",
    "\n",
    "    del afd_aspen\n",
    "    \n",
    "    #############################################\n",
    "    # Remove fires with less than 10 observations\n",
    "\n",
    "    # Get a count per fire\n",
    "    afd_counts = afd_aspen_f.groupby('fired_id').size().reset_index(name='counts')\n",
    "\n",
    "    # Get a list of IDs of fires with > 1 obs.\n",
    "    ids = afd_counts[afd_counts[\"counts\"] > 1]\n",
    "    \n",
    "    # Grab the new list of unique FIRED ids\n",
    "    ids = ids['fired_id'].unique().tolist()\n",
    "    \n",
    "    # Filter the datasets based on these FIRED ids\n",
    "    afd_aspen_f = afd_aspen_f[afd_aspen_f['fired_id'].isin(ids)]\n",
    "    fired_aspen_f = fired_aspen[fired_aspen['fired_id'].isin(ids)]\n",
    "\n",
    "    print(f\"Minimum obs./fire: {afd_counts['counts'].min()}; \\nMaximum obs./fire: {afd_counts['counts'].max()}\")\n",
    "    print(f\"Number of fires after filtering: {len(fired_aspen_f)}\")\n",
    "    print(f\"Number of obs. after filtering: {len(afd_aspen_f)}\")\n",
    "\n",
    "    del afd_counts, ids\n",
    "    \n",
    "    #################################################\n",
    "    # Plot the distribution of observations over time\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    afd_aspen_f['ACQ_DATE'].hist(bins=100)\n",
    "    plt.title(f'{key} - AFDs (2018-2023)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Observations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    ##############################\n",
    "    # Append to the new dictionary\n",
    "    gdfs[key] = afd_aspen_f\n",
    "\n",
    "print(f\"Total elapsed time: {round((time.time() - t0))} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b147fcb-77cc-409f-8d9b-174e66c46079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the files as they are currently\n",
    "gdfs['MOD61'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/mod61_archive_afd_aspen.gpkg'))\n",
    "gdfs['SNPP'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/snpp_archive_afd_aspen.gpkg'))\n",
    "gdfs['NOAA-20'].to_file(os.path.join(projdir,'data/spatial/mod/AFD/noaa20_archive_afd_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110adb8e-2c58-43c0-9238-3b23a36295a8",
   "metadata": {},
   "source": [
    "## Handling acquisition time-of-day and spatially overlapping observations\n",
    "\n",
    "The VIIRS S-NPP AFD have many overlapping observations during a single fire event. In some cases, the overlapping points are on the same day and time but with different FRP values. In these cases, it may be best to \n",
    "\n",
    "From here on we can work with just the SNPP because it has the most consistency across our time period (NOAA-20 started in 2020). Later we can investigate the combination of the three datasets or at least make a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740dcace-364d-4410-87c3-3729183d9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the SNPP observations\n",
    "\n",
    "snpp_aspen = gdfs['SNPP']\n",
    "\n",
    "# Filter out FRP == 0\n",
    "snpp_aspen = snpp_aspen[snpp_aspen['FRP'] > 0]\n",
    "snpp_aspen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58414-b092-4c75-8ac2-d3c60de9fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snpp_aspen['FRP'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698273-0922-4691-95cd-04d69c2a9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snpp_aspen['ACQ_TIME'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef843b08-b195-4b5f-811c-4743951e3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snpp_aspen = snpp_aspen.reset_index()\n",
    "snpp_aspen = snpp_aspen.rename(columns={'index':'index_'})\n",
    "snpp_aspen = snpp_aspen.drop(['index_right'], axis=1)\n",
    "print(snpp_aspen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba61900-2a4d-40cb-9e30-a206f07ad50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime object as a new column (in UTC)\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Function to convert ACQ_DATE and ACQ_TIME to a datetime object in UTC\n",
    "def convert_to_datetime(acq_date, acq_time):\n",
    "    # Ensure ACQ_TIME is in HHMM format\n",
    "    if len(acq_time) == 3:\n",
    "        acq_time = '0' + acq_time\n",
    "    elif len(acq_time) == 2:\n",
    "        acq_time = '00' + acq_time\n",
    "    elif len(acq_time) == 1:\n",
    "        acq_time = '000' + acq_time\n",
    "\n",
    "    acq_date_str = acq_date.strftime('%Y-%m-%d')\n",
    "    dt = datetime.strptime(acq_date_str + acq_time, '%Y-%m-%d%H%M')\n",
    "    dt_utc = pytz.utc.localize(dt)  # Localize the datetime object to UTC\n",
    "    return dt_utc\n",
    "\n",
    "# Apply the conversion function to create a new datetime column\n",
    "snpp_aspen.loc[:, 'ACQ_DATETIME'] = snpp_aspen.apply(lambda row: convert_to_datetime(row['ACQ_DATE'], row['ACQ_TIME']), axis=1)\n",
    "\n",
    "# Print the resulting GeoDataFrame with timezone-aware datetime objects\n",
    "print(snpp_aspen['ACQ_DATETIME'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da4785-d857-4fb3-9021-fb0b2dcae55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial intersection to identify overlapping observations\n",
    "overlap = gpd.sjoin(snpp_aspen, snpp_aspen, predicate='intersects', lsuffix='left', rsuffix='right')\n",
    "print(overlap.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df74c78-53a7-46f6-bbbe-fe7333ac18ef",
   "metadata": {},
   "source": [
    "#### Case 1: Same day/time observations with different FRP values (spatially overlapping)\n",
    "\n",
    "In this case, we have overlapping observations which have the same datetime but (sometimes) different FRP values. To handle this, we can group observations by datetime and perform a dissolve, taking the mean FRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078856f-4607-4b00-b377-3534292e1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique group ID for each set of intersecting observations with the same DATETIME\n",
    "overlap['_VID_'] = overlap.groupby(['ACQ_DATETIME_left', 'ACQ_DATETIME_right']).ngroup()\n",
    "print(overlap[['ACQ_DATETIME_left', 'ACQ_DATETIME_right','_VID_']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f164f-d9c4-4fd9-9f9f-28e8797b74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 90th percentile of VPD among the observations\n",
    "\n",
    "# Join the new _VID_ back to the original dataframe using VID\n",
    "snpp_aspen_ = snpp_aspen.merge(\n",
    "    overlap[['VID_left', '_VID_']].drop_duplicates(), \n",
    "    left_on='VID', right_on='VID_left', how='left').reset_index(drop=True)\n",
    "\n",
    "# Calculate the 90th percentile FRP for each _VID_\n",
    "def pct90(group):\n",
    "    group['FRP_90'] = np.percentile(group['FRP'], 90)\n",
    "    return group\n",
    "\n",
    "# Apply the function to calculate the 90th percentile FRP\n",
    "snpp_aspen_ = snpp_aspen_.groupby('_VID_').apply(pct90).reset_index(drop=True)\n",
    "snpp_aspen_[['_VID_','VID','ACQ_DATETIME','LATITUDE','LONGITUDE','FRP','FRP_90','VERSION']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7470e4-6ad2-41b2-a085-dc3e9fc309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve by the same day/time VID to create a new geometry with the 90th percentile FRP\n",
    "\n",
    "snpp_aspen_dis = snpp_aspen_.dissolve(by='_VID_').reset_index() # this takes the first of each, which should be OK\n",
    "\n",
    "snpp_aspen_dis.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdee6ea-fb9b-4df6-a0c6-9b5fa9a2f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out a version of these data\n",
    "\n",
    "# Create the buffered VIIRS obs.\n",
    "snpp_aspen_plot = snpp_aspen_dis.copy()\n",
    "snpp_aspen_plot['geometry'] = snpp_aspen_plot.geometry.buffer(375, cap_style=3)  # square buffer 375m\n",
    "\n",
    "# Save the VIIRS observations (points)\n",
    "snpp_aspen_dis = snpp_aspen_dis.to_crs(proj)\n",
    "snpp_aspen_dis.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_snpp_pt_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (plots)\n",
    "snpp_aspen_plot = snpp_aspen_plot.to_crs(proj)\n",
    "snpp_aspen_plot.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_snpp_plot_fired_events_west_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aceb58b-e1dd-40e3-9bb7-0fe102274ade",
   "metadata": {},
   "source": [
    "## Tidy the FRP data: remove null values, check on the obs./fire, and check on date matches\n",
    "\n",
    "Some observations may not be joined correctly (i.e., spatial overlap but wrong ignition year, etc). We may also have some fires with too few observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07377daf-08a7-4aef-9879-74ca09672a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the observation counts again\n",
    "viirs_counts = snpp_aspen_dis.groupby('fired_id').size().reset_index(name='counts')\n",
    "print(viirs_counts['counts'].min())\n",
    "print(viirs_counts['counts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c877-e71a-4894-9eee-0d33313752ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a map of the fire with the most observations\n",
    "\n",
    "# Sort the VIIRS counts\n",
    "viirs_counts = viirs_counts.sort_values('counts', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Take the first row (the maximum)\n",
    "max_obs = viirs_counts.iloc[0]['fired_id']\n",
    "print(max_obs)\n",
    "\n",
    "# Filter the fire perimeter and VIIRS obs.\n",
    "perim = fired_aspen[fired_aspen['fired_id'] == max_obs]\n",
    "obs = snpp_aspen_dis[snpp_aspen_dis['fired_id'] == max_obs]\n",
    "obs = obs.copy()\n",
    "obs['FRP_log'] = np.log1p(obs['FRP'])\n",
    "obs = obs[obs['DAYNIGHT'] == 'D']\n",
    "print(len(obs))\n",
    "\n",
    "# Create the map\n",
    "fig, ax = plt.subplots(figsize=(4, 5.5))\n",
    "# Plot VIIRS points\n",
    "obs.plot(column='FRP_log', ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Fire Radiative Power (FRP)\", 'orientation': \"horizontal\"},\n",
    "         cmap='magma', markersize=1, alpha=0.7)\n",
    "# Plot the fire perimeter\n",
    "perim.plot(ax=ax, color='none', edgecolor='black', linewidth=1, label='Fire Perimeter')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(maindir,'aspen-fire/Aim2/figures/FigX_MullenFire_FRP.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327045b-d443-41f9-ad51-ca789ebd4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frp_aspen_f['pct_aspen']))\n",
    "print(len(frp_aspen_f['FRP']))\n",
    "      \n",
    "# Scatterplot of FRP and aspen_pct (fire perimeter)\n",
    "plt.figure(figsize=(6, 4))  # Set the figure size\n",
    "plt.scatter(frp_aspen_f['pct_aspen'], frp_aspen_f['FRP'], alpha=0.5)  # Plot with some transparency\n",
    "\n",
    "# Add titles and labels\n",
    "plt.ylabel('Fire Radiative Power (FRP)')\n",
    "plt.xlabel('Aspen %')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812d666-ca41-46f6-9cfc-c9f7daf2d0df",
   "metadata": {},
   "source": [
    "## Join VIIRS observations to daily FIRED perimeters\n",
    "\n",
    "We want to summarize VIIRS observations on a daily basis and then associate them with the correct daily polygon from FIRED. The initial step is to group observations by day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec32dc-5938-4419-8477-d757f4c84ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the daily summary of FRP for each wildfire event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2658bb-7a19-4582-878b-f102e188f5fe",
   "metadata": {},
   "source": [
    "## Create the VIIRS observation buffer (375m2)\n",
    "\n",
    "The archive VIIRS data is distributed as shapefiles with centroids representing the pixel center of a VIIRS observation. In order to assess characteristics within the VIIRS observations, we want to create a 375m2 buffer around the centroid locations to approximate the VIIRS pixel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc88d3-d1dc-4f62-afe2-bef5c98f6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the buffered VIIRS obs.\n",
    "frp_aspen_plot = frp_aspen_f.copy()\n",
    "frp_aspen_plot['geometry'] = frp_aspen_plot.geometry.buffer(375, cap_style=3)  # square buffer 375m\n",
    "\n",
    "print(len(fired_aspen))\n",
    "\n",
    "# Let's plot one fire using the FRP column to color the \"plots\"\n",
    "\n",
    "# Filter the fire perimeter and VIIRS obs.\n",
    "perim = fired_aspen[fired_aspen['fired_id'] == \"42306\"]  # Williams Fork Fire \"45811.0\"\n",
    "obs = frp_aspen_plot[frp_aspen_plot['fired_id'] == \"42306\"]\n",
    "obs = obs.copy()\n",
    "obs['FRP_log'] = np.log1p(obs['FRP'])\n",
    "obs = obs[obs['DAYNIGHT'] == 'D']  # plot only daytime observations\n",
    "print(len(obs))\n",
    "\n",
    "# Create the map\n",
    "fig, ax = plt.subplots(figsize=(4, 5.5))\n",
    "# Plot VIIRS points\n",
    "obs.plot(column='FRP_log', ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Fire Radiative Power (FRP)\"},\n",
    "         cmap='magma', markersize=1, alpha=0.7)\n",
    "# Plot the fire perimeter\n",
    "perim.plot(ax=ax, color='none', edgecolor='black', linewidth=1, label='Fire Perimeter')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5ab75-9702-49e2-abb5-e92b3339605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = fired_aspen.copy()\n",
    "centroid['geometry'] = centroid.geometry.centroid\n",
    "\n",
    "# Make a spatial map of the centroids now\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "states.plot(ax=ax, edgecolor='black', linewidth=1, color='none')\n",
    "\n",
    "# Plot centroids\n",
    "centroid['size'] = centroid['pct_aspen'] * 10  # Adjust the scaling factor as necessary\n",
    "centroid.plot(\n",
    "    ax=ax, markersize=centroid['pct_aspen'], \n",
    "    column='pct_aspen', cmap='viridis', \n",
    "    legend=True, alpha=0.6, \n",
    "    legend_kwds={'label': \"Aspen Percent\"})\n",
    "\n",
    "# Optional: Plot the original fire perimeters for context\n",
    "fired_aspen.plot(ax=ax, color='none', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "\n",
    "del centroid\n",
    "\n",
    "# Save the map as a PNG\n",
    "plt.savefig(os.path.join(maindir,'aspen-fire/Aim2/figures/Fig1_aspen_fires.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bc2d2-ad96-45ad-ba17-cde5fdbf4db8",
   "metadata": {},
   "source": [
    "## Tidy and save out the necessary files\n",
    "\n",
    "Now that we have a tidy dataframe for both wildfires with >=5% pre-fire aspen cover and their associated nominal or high confidence VIIRS observations, we can save these files out for further processing. \n",
    "\n",
    "Some of the processing will occur in GEE, so for these files we want to save a simplified SHP with only the required attribute information (they will be joined back to the full data after processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96011e35-054b-4e03-b57d-d729af6828a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the observation counts again\n",
    "viirs_counts = frp_aspen_plot.groupby('fired_id').size().reset_index(name='counts')\n",
    "print(viirs_counts['counts'].min())\n",
    "print(viirs_counts['counts'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71248f-2f9c-4e16-8d22-e0f81c299d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the daily files\n",
    "# Get the list of IDs\n",
    "ids = fired_aspen['fired_id'].unique()\n",
    "\n",
    "# Load the daily polygons, subset to aspen fires\n",
    "daily['id'] = daily['id'].astype(str)\n",
    "daily = daily[daily['id'].isin(ids)]\n",
    "print(len(daily['id'].unique()))\n",
    "\n",
    "# Save the daily wildfire perimeters\n",
    "daily = daily.to_crs(proj)  # ensure the correct projection before exporting\n",
    "daily.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_daily_west_aspen.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b85319-68dc-4899-b7ed-977424bd8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wildfire perimeters\n",
    "fired_aspen = fired_aspen.to_crs(proj)  # ensure the correct projection before exporting\n",
    "fired_aspen.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/FIRED/fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (points)\n",
    "frp_aspen_f = frp_aspen_f.to_crs(proj)\n",
    "frp_aspen_f.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_obs_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Save the VIIRS observations (plots)\n",
    "frp_aspen_plot = frp_aspen_plot.to_crs(proj)\n",
    "frp_aspen_plot.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/VIIRS/viirs_plots_fired_events_west_aspen.gpkg'))\n",
    "\n",
    "# Tidy the files for GEE imports\n",
    "\n",
    "# FIRED perimeters (1km buffer)\n",
    "print(fired_aspen_1km.columns)\n",
    "fired_aspen_gee = fired_aspen_1km[['fired_id','ig_date','ig_year','last_date','mx_grw_dte','geometry']]\n",
    "fired_aspen_gee['ig_date'] = fired_aspen_gee['ig_date'].astype(str)\n",
    "fired_aspen_gee['last_date'] = fired_aspen_gee['ig_date'].astype(str)\n",
    "fired_aspen_gee.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/GEE/fired_events_west_aspen.shp'))\n",
    "\n",
    "# VIIRS \"plots\"\n",
    "print(frp_aspen_plot.columns)\n",
    "frp_aspen_gee = frp_aspen_plot[['fired_id','VID','ACQ_DATE','DAYNIGHT','geometry']]\n",
    "frp_aspen_gee['ACQ_DATE'] = frp_aspen_gee['ACQ_DATE'].astype(str)\n",
    "frp_aspen_gee.to_file(os.path.join(maindir,'aspen-fire/Aim2/data/spatial/mod/GEE/viirs_plots_fired_events_west_aspen.shp'))\n",
    "\n",
    "print(\"Success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
