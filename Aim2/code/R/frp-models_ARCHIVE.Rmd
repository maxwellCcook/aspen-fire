---
title: "frp-models"
output: html_document
date: "2023-07-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)

getwd()

proj <- st_crs("ESRI:102039")

# Tidy the model data frame (daily FRP observations)

# Spatial data
frp.sp <- st_read("../../data/spatial/mod/vnp14img_west_spatial_w_attr.gpkg") %>%
 # tidy the frame, select model coefficients
 select(
  id,mtbs_id,acq_date,frp,aspen_pct,cbd_mn,cbh_mn,
  vs_max,bi_max,vpd_max,elev,slope,roughness,
  evi_mn,pct_tree,pct_notree_veg,pct_noveg,
  geom
 ) %>%
 # Convert back to centroid (points)
 st_centroid() %>%
 st_transform(proj)

# Tabular format
frp.df <- frp.sp %>% st_set_geometry(NULL)

```

## Setting up the model

# Generate the Inverse Probability Weight (IPW) weights for aspen percent cover within FRP observations

Following methods of causal inference in ecology, we will generate Inverse Probability Weights (IPW) for the continuous treatment (aspen percentage). Because we are running the models for each fire individually, we need to generate the IPW values for individual fires.

```{r}

# Center and scale the variables
frp.df <- frp.df %>%       
  mutate_at(c("cbd_mn","cbh_mn","evi_mn",
              "vs_max","bi_max","vpd_max",
              "elev","slope","roughness",
              "pct_tree","pct_notree_veg","pct_noveg"), 
            ~(scale(.) %>% as.vector))

# Loop through wildfires, calculating the IPW
dfs <- list()
fires <- unique(frp.df$mtbs_id)
for (i in 1:length(fires)) {

 # Get the fire data frame, y and x variables
 df <- frp.df %>% filter(mtbs_id == fires[i]) %>% drop_na()

 num <- 0
 den <- 0
 
 # Calculate the IPW for the fire
 # Calculate the numerator (expected distribution of aspen %)
 model_num <- lm(aspen_pct ~ 1, data = df)
 num <- dnorm(df$aspen_pct, predict(model_num), sd(model_num$residuals))
 
 # Now calculate the denominator (exp. distribution regressed against confounders)
 model_den <- lm(
  aspen_pct ~ cbd_mn + cbh_mn + vs_max + bi_max + vpd_max + 
   elev + slope + roughness + evi_mn +
   pct_tree + pct_notree_veg + pct_noveg, data=df)
 den <- dnorm(df$aspen_pct, predict(model_den), sd(model_den$residuals))
 # Add the IPW column to the data frame
 df <- df %>% mutate(ipw = num / den)
 
 dfs[[i]] <- df
}

# Bind the rows back together
frp.df <- bind_rows(dfs)
glimpse(frp.df)

# Tidy up
rm(model_num,model_den,num,den,df,dfs,i)

```

Now we need to generate a weights matrix for each fire event. We can do this in parallel ...

```{r}
# Set up the parallel computing
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores-1)
doParallel::registerDoParallel(cl)
```

Set up the function for generating the spatial weights matrix and mSAR model:

```{r}

library(sp,quietly=T)
library(spdep,quietly=T)
library(spatialreg,quietly=T)

sar_model_ipw <- function(df) {
 # Create the weights matrix
 df.sp <- as_Spatial(df)
 coords <- coordinates(df.sp) # grab the coordinates
 ids <- row.names(as(df.sp,"data.frame")) # grab the ids
 list_nb <- tri2nb(coords, row.names=ids)
 W <- nb2listw(list_nb, style="W", zero.policy=FALSE) # the weights matrix
 # Set up the model
 model.sar <- 
  spautolm(log(frp) ~ aspen_pct + cbd_mn + cbh_mn + evi_mn + elev + slope + roughness + 
            vs_max + bi_max + vpd_max + pct_tree + pct_notree_veg + pct_noveg, 
         data=df.sp@data, 
         listw = W, 
         family = "SAR", 
         method="MC", 
         weights = ipw)
 # Store the model object
 
}

```

```{r}
results <- split(frp.df, frp.df$mtbs_id) # split the data frame into groups by fire id
```

# Test for spatial dependencies in the frp observations using distance matrix

```{r}

library(sp,quietly=T)
library(spdep,quietly=T)

# Create the spatial weights matrix

spat.df <- as_Spatial(mod.df)

coords <- coordinates(spat.df)
IDs <- row.names(as(spat.df, "data.frame"))

list_nb <- tri2nb(coords, row.names=IDs)
W <- nb2listw(list_nb, style="W", zero.policy=FALSE)
W
plot(W,coords)

rm(list_nb,IDs)

```

Spatial weight matrices from KNN approach at various neighborhood sizes:

```{r}
# First round of KNN
knn1 <- knearneigh(coords, k = 5, longlat = TRUE)
str(knn1)
# convert to nb
k1 <- knn2nb(knn1)
# get the critical threshold
critical.threshold <- max(unlist(nbdists(k1,coords)))
critical.threshold
# get the distance band list
nb.dist.band <- dnearneigh(coords, 0, critical.threshold)
summary(nb.dist.band)
# Plot neighborhood distributions
dist.band.card <- card(nb.dist.band)
dist.band.card
ggplot() +
  geom_histogram(aes(x=dist.band.card)) +
  xlab("Number of Neighbors")
# grab the listw
Wknn <- nb2listw(nb.dist.band, style="W", zero.policy=FALSE)
Wknn
```

Test for spatial dependency in the data frame ...

First, run an OLS regression without the IPW and test for correlations using the VIF function:

```{r}

# Run the generalized OLS with IPW weights

model.ols <- lm(log(frp) ~ aspen_pct + lfcbd_avg + lfcbh_avg + 
                  chili + slope + vs_max + bi_max + vpd_max + pct_tree + pct_notree_veg,
               data = mod.df)
summary(model.ols)

#calculate the VIF for each predictor variable in the model
print("Variance Inflation Factor (VIF) results: ")
vif(model.ols)

#test for normality in the residuals
hist(model.ols$residuals, breaks = 20)

```

We can also look at the correlation plot:

```{r}

df <- mod.df %>% 
    st_set_geometry(NULL) %>% 
    as_tibble() %>%
    select(-c(GID,acq_date,LONGITUDE,LATITUDE,frp,ipw))

corr <- round(cor(df), 1)
p.mat <- cor_pmat(df)

corrp <- ggcorrplot(corr, hc.order = TRUE,
           type = "lower", p.mat = p.mat,
           lab = TRUE, 
           colors = c("#ca0020", "#f7f7f7", "#0571b0")) +
  theme_bw(8) +
  theme(axis.text.x = element_text(angle=35,hjust=1),
        axis.title = element_blank())
corrp

ggsave(corrp,file="../../figures/frp/correlations.png",dpi=150)

rm(corr,p.mat,df)

```

Seems like we might be OK in terms of correlated predictors. Now run the OLS with the IPW weights:

```{r}

model.ols_ipw <- lm(log(frp) ~ aspen_pct + lfcbd_avg + lfcbh_avg + 
                  chili + slope + vs_max + bi_max + vpd_max + pct_notree_veg + pct_tree,
               data = mod.df, weights = ipw)
summary(model.ols_ipw)

# # compare with anova
# print("Results from ANOVA: ")
# tidy(anova(model.ols, model.ols_ipw))
# hist(model.ols$residuals, breaks = 20)
# plot(model.ols)

```

Plot the spatially lagged frp:

```{r}

frp.lag <- lag.listw(W, mod.df$frp)

plot(frp.lag ~ mod.df$frp, pch=16, asp=1)
abline(lm(frp.lag ~ mod.df$frp), col="blue")

```

Now run the Moran's I test on the weighted model:

```{r}
# Run the Moran's I test
moran.lm <- lm.morantest(model.ols_ipw, W, alternative="two.sided")
print(moran.lm)
if (moran.lm$p.value < 0.5) {
  print("Null rejected, spatial dependency exists ...")
}

# Moran plot
moran.plot(log(mod.df$frp),W)

# Lm test
lmLMtests <- lm.LMtests(model.ols_ipw, W, test="all")
lmLMtests

# Tidy up
rm(moran.lm, lmLMtests)
```

Plot the residuals from the OLS model:

```{r}

df <- frp %>% 
  mutate(olsresids = resid(model.ols_ipw))
glimpse(df)

hist(df$olsresids)

ggplot(data=df) +
  geom_sf(aes(color=olsresids)) +
  scale_color_steps(
    name = "OLS Residuals",
    low = "lightsteelblue1",
    high = "tomato1",
    n.breaks = 7,
    show.limits = T) +
  theme_void()

rm(df)

```

We have high spatial dependence in simple OLS regression, indicating that we should use a spatial model.

Start with the most basic SAR model with IPW weights and spatial weights matrix:

```{r}

library(spatialreg,quietly=T)

model.sar <- spautolm(log(frp) ~ aspen_pct + lfcbd_avg + lfcbh_avg + chili + slope + vs_max + bi_max
                                  + vpd_max + pct_tree + pct_notree_veg, 
         data=spat.df@data, 
         listw = W, 
         family = "SAR", 
         method="MC", 
         weights = ipw)

summary(model.sar,Nagelkerke=T,adj.se=T)
coef(model.sar)
AIC(model.ols,model.ols_ipw,model.sar)

```

Check out some of the model parameters:

```{r}
resids <- residuals(model.sar)
hist(resids)
moran.test(resids,listw = W)
```

Plot the effects:

```{r}

sar.effects <- data.frame(coef(model.sar)) %>%
  rename(effect = coef.model.sar.) %>%
  mutate(sign = if_else(effect<0,"negative","positive"))
sar.effects <- tibble::rownames_to_column(sar.effects, "variable")
(sar.effects)

sar.effects$variable

sar.effects <- sar.effects %>% filter(variable != "(Intercept)" & variable != "lambda")

# Reorder band factors
sar.effects$variable <- factor(sar.effects$variable, 
                               levels = c("pct_tree","pct_notree_veg","vpd_max","bi_max","vs_max",
                                          "slope","chili","lfcbh_avg","lfcbd_avg","aspen_pct"))
  
sar.eff.plot <- ggplot(data=sar.effects,
                       aes(y=variable, x=effect, color=sign,label=round(effect,2))) +
  geom_point() +
  geom_text(vjust="inward",hjust="inward") +
  geom_vline(xintercept=0,color="grey60",size=0.4) +
  labs(x="Estimate") +
  theme_bw(9) +
  theme(legend.position = "none",
        axis.title.y = element_blank())
sar.eff.plot

```

Look at estimates from all models:

```{r}

library(sjPlot)
theme_sjplot(base_size = 7, base_family = "")

sjPlot::tab_model(model.ols, 
                  show.re.var= TRUE)

sjPlot::tab_model(model.ols_ipw, 
                  show.re.var= TRUE)

arrplot <- cowplot::plot_grid(
  plot_model(model.ols,show.values = TRUE, value.offset = .3, vline.color="grey60", title="") + theme_bw(9),
  plot_model(model.ols_ipw,show.values = TRUE, value.offset = .3, vline.color="grey60",title="") + theme_bw(9),
  sar.eff.plot,
  labels = c('A', 'B', 'C'),
  nrow=1, ncol=3
)
arrplot

ggsave(arrplot, file="../../figures/frp/model_effects_grid.png", width=9, height=4)

```


Set up the equation to generate Inverse Probability Weights (IPW) for aspen percentage:

```{r}

# The numerator is the probability distribution of just the treatment variable.
# We'll use a normal distribution for it (hence dnorm()). We need to feed
# dnorm() the grant amount for each person, the predicted value from a simple
# grant ~ 1 model, and the sd of the residuals from that model
model_num <- glm(aspen_prop_c ~ 1, data = model.df_sc)
num <- dnorm(model.df_sc$aspen_prop_c,
             predict(model_num),
             sd(model_num$residuals))

# The denominator is the probability distribution of the treatment variable
# explained by the confounders. We'll again use a normal distribution for it.
# We'll feed dnorm() the grant amount, the predicted value from a model that
# includes the confounders, and the sd of the residuals from that model
model_den <- glm(aspen_prop_c ~ lfcbh_avg + vs_p90 + 
    vpd_p90 + fm1000_p90 + elev_sd + aspect_mean + modis_pct_notree_veg, 
                 data = model.df_sc)
den <- dnorm(model.df_sc$aspen_prop_c,
             predict(model_den),
             sd(model_den$residuals))

model.df_ipw <- model.df_sc %>% 
  mutate(ipw = num / den)

head(model.df_ipw)

rm(num,model_num,model_den)

```

Look at estimates of both models:

Check assumptions:

```{r}
plot(model)
plot(model.ipw)
```

GLMM with nested effects for date/plot:

```{r}

model.df_ipw$day <- as.numeric(model.df_ipw$acq_date)

library(lme4,quietly=T)

model.ipw.glmm <- lmer(log(frp) ~ aspen_prop_c + lfcbh_avg + vs_p90 + 
                          vpd_p90 + fm1000_p90 + elev_sd + aspect_mean + modis_pct_notree_veg + (1|day),
                        data = model.df_ipw, weights=ipw)
summary(model.ipw.glmm)

```

```{r}
cowplot::plot_grid(
  plot_model(model.ipw.glmm,show.values = TRUE, value.offset = .3)+theme_minimal(10),
  plot_model(model.ipw.glmm, "re")+theme_minimal(10)
) +
  theme_bw(10)
```

```{r}
effects_aspen <- effects::effect(term= "aspen_prop_c", mod= model.ipw.glmm)
summary(effects_aspen) #output of what the values are
effects_aspen <- as.data.frame(effects_aspen)
```

```{r}
sjPlot::tab_model(model.ipw.glmm, 
                  show.re.var= TRUE)
```

## SAR Model

Time-series:

```{r}

ts.df <- model.df %>%
  st_set_geometry(NULL) %>% as_tibble() %>%
  select(-c(LATITUDE,LONGITUDE,GID,aspen_noaspen)) %>%
  group_by(acq_date) %>%
  summarize_all(list(mean))

ts.df <- ts.df %>%       
  mutate_at(c("aspen_prop_c", "lfcbd_avg", "lfcbh_avg", "vs_p90", "tmmx_p90",
              "bi_p90", "vpd_p90", "fm1000_p90", "elev_sd", "slope_mean",
              "aspect_mean", "modis_pct_notree_veg", "modis_pct_noveg"), 
            ~(scale(.) %>% as.vector))
glimpse(ts.df)

# # convert to multivariate time-series
# ts.df <- xts(ts.df[,-1], order.by=as.POSIXct(ts.df$acq_date))

```

Create the spatial weights matrix based on nearest neighbor:

```{r}

library(spdep)

coords<-cbind(model.df$LONGITUDE,model.df$LATITUDE)
coords<-as.matrix(coords)

nb <- dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
nb.w <- nb2listw(nb, glist=NULL, style="W", zero.policy=FALSE)

```

SAR model:

```{r}

library(spatialreg)
sar.chi <- lagsarlm(log(frp)~ aspen_prop_c + lfcbh_avg + vs_p90 + 
                      vpd_p90 + fm1000_p90 + elev_sd + aspect_mean + modis_pct_notree_veg + 
                      modis_pct_noveg, 
                    data=model.df, nb.w)
summary(sar.chi)

```

